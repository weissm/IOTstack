{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IOT Stack IOTstack is a builder for docker-compose to easily make and maintain IoT stacks on the Raspberry Pi. Documentation for the project: https://sensorsiot.github.io/IOTstack/ Video Andreas Spiess | #295 Raspberry Pi Server based on Docker, with VPN, Dropbox backup, Influx, Grafana, etc. Installation On the (RPi) lite image you will need to install git first sudo apt-get install git -y Download the repository with: git clone https://github.com/SensorsIot/IOTstack.git ~/IOTstack Due to some script restraints, this project needs to be stored in ~/IOTstack To enter the directory and run menu for installation options: cd ~/IOTstack && bash ./menu.sh Install docker with the menu, restart your system. Run menu again to select your build options, then start docker-compose with docker-compose up -d Experimental Features Want to have the latest and greatest features? Switch to the experimental branch: git pull && git checkout experimental ./menu.sh Do note that the experimental branch may be broken, or may break your setup, so ensure you have a good backup, and please report any issues. Migrating from the old repo? cd ~/IOTstack/ git remote set-url origin https://github.com/SensorsIot/IOTstack.git git pull origin master docker-compose down ./menu.sh docker-compose up -d Add to the project Feel free to add your comments on features or images that you think should be added. Contributions If you use some of the tools in the project please consider donating or contributing on their projects. It doesn't have to be monetary, reporting bugs and PRs help improve the projects for everyone.","title":"IOT Stack"},{"location":"#iot-stack","text":"IOTstack is a builder for docker-compose to easily make and maintain IoT stacks on the Raspberry Pi.","title":"IOT Stack"},{"location":"#documentation-for-the-project","text":"https://sensorsiot.github.io/IOTstack/","title":"Documentation for the project:"},{"location":"#video","text":"Andreas Spiess | #295 Raspberry Pi Server based on Docker, with VPN, Dropbox backup, Influx, Grafana, etc.","title":"Video"},{"location":"#installation","text":"On the (RPi) lite image you will need to install git first sudo apt-get install git -y Download the repository with: git clone https://github.com/SensorsIot/IOTstack.git ~/IOTstack Due to some script restraints, this project needs to be stored in ~/IOTstack To enter the directory and run menu for installation options: cd ~/IOTstack && bash ./menu.sh Install docker with the menu, restart your system. Run menu again to select your build options, then start docker-compose with docker-compose up -d","title":"Installation"},{"location":"#experimental-features","text":"Want to have the latest and greatest features? Switch to the experimental branch: git pull && git checkout experimental ./menu.sh Do note that the experimental branch may be broken, or may break your setup, so ensure you have a good backup, and please report any issues.","title":"Experimental Features"},{"location":"#migrating-from-the-old-repo","text":"cd ~/IOTstack/ git remote set-url origin https://github.com/SensorsIot/IOTstack.git git pull origin master docker-compose down ./menu.sh docker-compose up -d","title":"Migrating from the old repo?"},{"location":"#add-to-the-project","text":"Feel free to add your comments on features or images that you think should be added.","title":"Add to the project"},{"location":"#contributions","text":"If you use some of the tools in the project please consider donating or contributing on their projects. It doesn't have to be monetary, reporting bugs and PRs help improve the projects for everyone.","title":"Contributions"},{"location":"Accessing-your-Device-from-the-internet/","text":"Accessing your device from the internet The challenge most of us face with remotely accessing your home network is that you don't have a static IP. From time to time the IP that your ISP assigns to you changes and it's difficult to keep up. Fortunately, there is a solution, a DynamicDNS. The section below shows you how to set up an easy to remember address that follows your public IP no matter when it changes. Secondly, how do you get into your home network? Your router has a firewall that is designed to keep the rest of the internet out of your network to protect you. Here we install a VPN and configure the firewall to only allow very secure VPN traffic in. DuckDNS If you want to have a dynamic DNS point to your Public IP I added a helper script. Register with duckdns.org and create a subdomain name. Then edit the nano ~/IOTstack/duck/duck.sh file and add your DOMAINS=\"YOUR_DOMAINS\" DUCKDNS_TOKEN=\"YOUR_DUCKDNS_TOKEN\" first test the script to make sure it works sudo ~/IOTstack/duck/duck.sh then cat /var/log/duck.log . If you get KO then something has gone wrong and you should check out your settings in the script. If you get an OK then you can do the next step. Create a cron job by running the following command crontab -e You will be asked to use an editor option 1 for nano should be fine paste the following in the editor */5 * * * * sudo ~/IOTstack/duck/duck.sh >/dev/null 2>&1 then ctrl+s and ctrl+x to save Your Public IP should be updated every five minutes PiVPN pimylifeup.com has an excellent tutorial on how to install PiVPN In point 17 and 18 they mention using noip for their dynamic DNS. Here you can use the DuckDNS address if you created one. Don't forget you need to open the port 1194 on your firewall. Most people won't be able to VPN from inside their network so download OpenVPN client for your mobile phone and try to connect over mobile data. ( More info. ) Once you activate your VPN (from your phone/laptop/work computer) you will effectively be on your home network and you can access your devices as if you were on the wifi at home. I personally use the VPN any time I'm on public wifi, all your traffic is secure. Zerotier https://www.zerotier.com/ Zerotier is an alternative to PiVPN that doesn't require port forwarding on your router. It does however require registering for their free tier service here . Kevin Zhang has written a how to guide here . Just note that the install link is outdated and should be: curl -s 'https://raw.githubusercontent.com/zerotier/ZeroTierOne/master/doc/contact%40zerotier.com.gpg' | gpg --import && \\ if z=$(curl -s 'https://install.zerotier.com/' | gpg); then echo \"$z\" | sudo bash; fi","title":"Accessing your device from the internet"},{"location":"Accessing-your-Device-from-the-internet/#accessing-your-device-from-the-internet","text":"The challenge most of us face with remotely accessing your home network is that you don't have a static IP. From time to time the IP that your ISP assigns to you changes and it's difficult to keep up. Fortunately, there is a solution, a DynamicDNS. The section below shows you how to set up an easy to remember address that follows your public IP no matter when it changes. Secondly, how do you get into your home network? Your router has a firewall that is designed to keep the rest of the internet out of your network to protect you. Here we install a VPN and configure the firewall to only allow very secure VPN traffic in.","title":"Accessing your device from the internet"},{"location":"Accessing-your-Device-from-the-internet/#duckdns","text":"If you want to have a dynamic DNS point to your Public IP I added a helper script. Register with duckdns.org and create a subdomain name. Then edit the nano ~/IOTstack/duck/duck.sh file and add your DOMAINS=\"YOUR_DOMAINS\" DUCKDNS_TOKEN=\"YOUR_DUCKDNS_TOKEN\" first test the script to make sure it works sudo ~/IOTstack/duck/duck.sh then cat /var/log/duck.log . If you get KO then something has gone wrong and you should check out your settings in the script. If you get an OK then you can do the next step. Create a cron job by running the following command crontab -e You will be asked to use an editor option 1 for nano should be fine paste the following in the editor */5 * * * * sudo ~/IOTstack/duck/duck.sh >/dev/null 2>&1 then ctrl+s and ctrl+x to save Your Public IP should be updated every five minutes","title":"DuckDNS"},{"location":"Accessing-your-Device-from-the-internet/#pivpn","text":"pimylifeup.com has an excellent tutorial on how to install PiVPN In point 17 and 18 they mention using noip for their dynamic DNS. Here you can use the DuckDNS address if you created one. Don't forget you need to open the port 1194 on your firewall. Most people won't be able to VPN from inside their network so download OpenVPN client for your mobile phone and try to connect over mobile data. ( More info. ) Once you activate your VPN (from your phone/laptop/work computer) you will effectively be on your home network and you can access your devices as if you were on the wifi at home. I personally use the VPN any time I'm on public wifi, all your traffic is secure.","title":"PiVPN"},{"location":"Accessing-your-Device-from-the-internet/#zerotier","text":"https://www.zerotier.com/ Zerotier is an alternative to PiVPN that doesn't require port forwarding on your router. It does however require registering for their free tier service here . Kevin Zhang has written a how to guide here . Just note that the install link is outdated and should be: curl -s 'https://raw.githubusercontent.com/zerotier/ZeroTierOne/master/doc/contact%40zerotier.com.gpg' | gpg --import && \\ if z=$(curl -s 'https://install.zerotier.com/' | gpg); then echo \"$z\" | sudo bash; fi","title":"Zerotier"},{"location":"Backups/","text":"Backups Because containers can easily be rebuilt from docker hub we only have to back up the data in the \"volumes\" directory. Cloud Backups Dropbox-Uploader This a great utility to easily upload data from your Pi to the cloud. https://magpi.raspberrypi.org/articles/dropbox-raspberry-pi. It can be installed from the Menu under Backups. rclone (Google Drive) This is a service to upload to Google Drive. The config is described here . Install it from the menu then follow the link for these sections: * Getting a Google Drive Client ID * Setting up the Rclone Configuration When naming the service in rclone config ensure to call it \"gdrive\" The Auto-mounting instructions for the drive in the link don't work on Rasbian . Auto-mounting of the drive isn't necessary for the backup script. If you want your Google Drive to mount on every boot then follow the instructions at the bottom of the wiki page Influxdb ~/IOTstack/scripts/backup_influxdb.sh does a database snapshot and stores it in ~/IOTstack/backups/influxdb/db . This can be restored with the help a script (that I still need to write) Docker backups The script ~/IOTstack/scripts/docker_backup.sh performs the master backup for the stack. This script can be placed in a cron job to backup on a schedule. Edit the crontab with crontab -e Then add 0 23 * * * ~/IOTstack/scripts/docker_backup.sh >/dev/null 2>&1 to have a backup every night at 23:00. This script cheats by copying the volume folder live. The correct way would be to stop the stack first then copy the volumes and restart. The cheating method shouldn't be a problem unless you have fast changing data like in influxdb. This is why the script makes a database export of influxdb and ignores its volume. Cloud integration The docker_backup.sh script now no longer requires modification to enable cloud backups. It now tests for the presence of and enable file in the backups folder Drobox-Uploader The backup tests for a file called ~/IOTstack/backups/dropbox , if it is present it will upload to dropbox. To disable dropbox upload delete the file. To enable run sudo touch ~/IOTstack/backups/dropbox rclone The backup tests for a file called ~/IOTstack/backups/rclone , if it is present it will upload to google drive. To disable rclone upload delete the file. To enable run sudo touch ~/IOTstack/backups/rclone Pruning online backups @877dev has added functionality to prune both local and cloud backups. For dropbox make sure you dont have any files that contain spaces in your backup directory as the script cannot handle it at this time. Restoring a backup The \"volumes\" directory contains all the persistent data necessary to recreate the container. The docker-compose.yml and the environment files are optional as they can be regenerated with the menu. Simply copy the volumes directory into the IOTstack directory, Rebuild the stack and start. Added your Dropbox token incorrectly or aborted the install at the token screen Make sure you are running the latest version of the project link . Run ~/Dropbox-Uploader/dropbox_uploader.sh unlink and if you have added it key then it will prompt you to confirm its removal. If no key was found it will ask you for a new key. Confirm by running ~/Dropbox-Uploader/dropbox_uploader.sh it should ask you for your key if you removed it or show you the following prompt if it has the key: $ ~/Dropbox-Uploader/dropbox_uploader.sh Dropbox Uploader v1.0 Andrea Fabrizi - andrea.fabrizi@gmail.com Usage: /home/pi/Dropbox-Uploader/dropbox_uploader.sh [PARAMETERS] COMMAND... Commands: upload <LOCAL_FILE/DIR ...> <REMOTE_FILE/DIR> download <REMOTE_FILE/DIR> [LOCAL_FILE/DIR] delete <REMOTE_FILE/DIR> move <REMOTE_FILE/DIR> <REMOTE_FILE/DIR> copy <REMOTE_FILE/DIR> <REMOTE_FILE/DIR> mkdir <REMOTE_DIR> .... Ensure you are not running as sudo as this will store your api in the /root directory as /root/.dropbox_uploader If you ran the command with sudo the remove the old token file if it exists with either sudo rm /root/.dropbox_uploader or sudo ~/Dropbox-Uploader/dropbox_uploader.sh unlink Auto-mount Gdrive with rclone To enable rclone to mount on boot you will need to make a user service. Run the following commands mkdir -p ~/.config/systemd/user nano ~/.config/systemd/user/gdrive.service Copy the following code into the editor, save and exit [Unit] Description=rclone: Remote FUSE filesystem for cloud storage Documentation=man:rclone(1) [Service] Type=notify ExecStartPre=/bin/mkdir -p %h/mnt/gdrive ExecStart= \\ /usr/bin/rclone mount \\ --fast-list \\ --vfs-cache-mode writes \\ gdrive: %h/mnt/gdrive [Install] WantedBy=default.target enable it to start on boot with: (no sudo) systemctl --user enable gdrive.service start with systemctl --user start gdrive.service if you no longer want it to start on boot then type: systemctl --user disable gdrive.service","title":"Backups"},{"location":"Backups/#backups","text":"Because containers can easily be rebuilt from docker hub we only have to back up the data in the \"volumes\" directory.","title":"Backups"},{"location":"Backups/#cloud-backups","text":"","title":"Cloud Backups"},{"location":"Backups/#dropbox-uploader","text":"This a great utility to easily upload data from your Pi to the cloud. https://magpi.raspberrypi.org/articles/dropbox-raspberry-pi. It can be installed from the Menu under Backups.","title":"Dropbox-Uploader"},{"location":"Backups/#rclone-google-drive","text":"This is a service to upload to Google Drive. The config is described here . Install it from the menu then follow the link for these sections: * Getting a Google Drive Client ID * Setting up the Rclone Configuration When naming the service in rclone config ensure to call it \"gdrive\" The Auto-mounting instructions for the drive in the link don't work on Rasbian . Auto-mounting of the drive isn't necessary for the backup script. If you want your Google Drive to mount on every boot then follow the instructions at the bottom of the wiki page","title":"rclone (Google Drive)"},{"location":"Backups/#influxdb","text":"~/IOTstack/scripts/backup_influxdb.sh does a database snapshot and stores it in ~/IOTstack/backups/influxdb/db . This can be restored with the help a script (that I still need to write)","title":"Influxdb"},{"location":"Backups/#docker-backups","text":"The script ~/IOTstack/scripts/docker_backup.sh performs the master backup for the stack. This script can be placed in a cron job to backup on a schedule. Edit the crontab with crontab -e Then add 0 23 * * * ~/IOTstack/scripts/docker_backup.sh >/dev/null 2>&1 to have a backup every night at 23:00. This script cheats by copying the volume folder live. The correct way would be to stop the stack first then copy the volumes and restart. The cheating method shouldn't be a problem unless you have fast changing data like in influxdb. This is why the script makes a database export of influxdb and ignores its volume.","title":"Docker backups"},{"location":"Backups/#cloud-integration","text":"The docker_backup.sh script now no longer requires modification to enable cloud backups. It now tests for the presence of and enable file in the backups folder","title":"Cloud integration"},{"location":"Backups/#drobox-uploader","text":"The backup tests for a file called ~/IOTstack/backups/dropbox , if it is present it will upload to dropbox. To disable dropbox upload delete the file. To enable run sudo touch ~/IOTstack/backups/dropbox","title":"Drobox-Uploader"},{"location":"Backups/#rclone","text":"The backup tests for a file called ~/IOTstack/backups/rclone , if it is present it will upload to google drive. To disable rclone upload delete the file. To enable run sudo touch ~/IOTstack/backups/rclone","title":"rclone"},{"location":"Backups/#pruning-online-backups","text":"@877dev has added functionality to prune both local and cloud backups. For dropbox make sure you dont have any files that contain spaces in your backup directory as the script cannot handle it at this time.","title":"Pruning online backups"},{"location":"Backups/#restoring-a-backup","text":"The \"volumes\" directory contains all the persistent data necessary to recreate the container. The docker-compose.yml and the environment files are optional as they can be regenerated with the menu. Simply copy the volumes directory into the IOTstack directory, Rebuild the stack and start.","title":"Restoring a backup"},{"location":"Backups/#added-your-dropbox-token-incorrectly-or-aborted-the-install-at-the-token-screen","text":"Make sure you are running the latest version of the project link . Run ~/Dropbox-Uploader/dropbox_uploader.sh unlink and if you have added it key then it will prompt you to confirm its removal. If no key was found it will ask you for a new key. Confirm by running ~/Dropbox-Uploader/dropbox_uploader.sh it should ask you for your key if you removed it or show you the following prompt if it has the key: $ ~/Dropbox-Uploader/dropbox_uploader.sh Dropbox Uploader v1.0 Andrea Fabrizi - andrea.fabrizi@gmail.com Usage: /home/pi/Dropbox-Uploader/dropbox_uploader.sh [PARAMETERS] COMMAND... Commands: upload <LOCAL_FILE/DIR ...> <REMOTE_FILE/DIR> download <REMOTE_FILE/DIR> [LOCAL_FILE/DIR] delete <REMOTE_FILE/DIR> move <REMOTE_FILE/DIR> <REMOTE_FILE/DIR> copy <REMOTE_FILE/DIR> <REMOTE_FILE/DIR> mkdir <REMOTE_DIR> .... Ensure you are not running as sudo as this will store your api in the /root directory as /root/.dropbox_uploader If you ran the command with sudo the remove the old token file if it exists with either sudo rm /root/.dropbox_uploader or sudo ~/Dropbox-Uploader/dropbox_uploader.sh unlink","title":"Added your Dropbox token incorrectly or aborted the install at the token screen"},{"location":"Backups/#auto-mount-gdrive-with-rclone","text":"To enable rclone to mount on boot you will need to make a user service. Run the following commands mkdir -p ~/.config/systemd/user nano ~/.config/systemd/user/gdrive.service Copy the following code into the editor, save and exit [Unit] Description=rclone: Remote FUSE filesystem for cloud storage Documentation=man:rclone(1) [Service] Type=notify ExecStartPre=/bin/mkdir -p %h/mnt/gdrive ExecStart= \\ /usr/bin/rclone mount \\ --fast-list \\ --vfs-cache-mode writes \\ gdrive: %h/mnt/gdrive [Install] WantedBy=default.target enable it to start on boot with: (no sudo) systemctl --user enable gdrive.service start with systemctl --user start gdrive.service if you no longer want it to start on boot then type: systemctl --user disable gdrive.service","title":"Auto-mount Gdrive with rclone"},{"location":"Custom/","text":"Custom services and overriding default settings for IOTstack You can specify modifcations to the docker-compose.yml file, including your own networks and custom containers/services. Create a file called compose-override.yml in the main directory, and place your modifications into it. These changes will be merged into the docker-compose.yml file next time you run the build script. The compose-override.yml file has been added to the .gitignore file, so it shouldn't be touched when upgrading IOTstack. It has been added to the backup script, and so will be included when you back up and restore IOTstack. Always test your backups though! New versions of IOTstack may break previous builds. How it works After the build process has been completed, a temporary docker compose file is created in the tmp directory. The script then checks if compose-override.yml exists: If it exists, then continue to step 3 If it does not exist, copy the temporary docker compose file to the main directory and rename it to docker-compose.yml . Using the yaml_merge.py script, merge both the compose-override.yml and the temporary docker compose file together; Using the temporary file as the default values and interating through each level of the yaml structure, check to see if the compose-override.yml has a value set. Output the final file to the main directory, calling it docker-compose.yml . A word of caution If you specify an override for a service, and then rebuild the docker-compose.yml file, but deselect the service from the list, then the YAML merging will still produce that override. For example, lets say NodeRed was selected to have have the following override specified in compose-override.yml : services: nodered: restart: always When rebuilding the menu, ensure to have NodeRed service always included because if it's no longer included, the only values showing in the final docker-compose.yml file for NodeRed will be the restart key and its value. Docker Compose will error with the following message: Service nodered has neither an image nor a build context specified. At least one must be provided. When attempting to bring the services up with docker-compose up -d . Either remove the override for NodeRed in compose-override.yml and rebuild the stack, or ensure that NodeRed is built with the stack to fix this. Examples Overriding default settings Lets assume you put the following into the compose-override.yml file: services: mosquitto: ports: - 1996:1996 - 9001:9001 Normally the mosquitto service would be built like this inside the docker-compose.yml file: version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1883:1883 - 9001:9001 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Take special note of the ports list. If you run the build script with the compose-override.yml file in place, and open up the final docker-compose.yml file, you will notice that the port list have been replaced with the ones you specified in the compose-override.yml file. version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1996:1996 - 9001:9001 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Do note that it will replace the entire list, if you were to specify services: mosquitto: ports: - 1996:1996 Then the final output will be: version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1996:1996 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Adding custom services Custom services can be added in a similar way to overriding default settings for standard services. Lets add a Minecraft and rcon server to IOTstack. Firstly, put the following into compose-override.yml : services: mosquitto: ports: - 1996:1996 - 9001:9001 minecraft: image: itzg/minecraft-server ports: - \"25565:25565\" volumes: - \"./volumes/minecraft:/data\" environment: EULA: \"TRUE\" TYPE: \"PAPER\" ENABLE_RCON: \"true\" RCON_PASSWORD: \"PASSWORD\" RCON_PORT: 28016 VERSION: \"1.15.2\" REPLACE_ENV_VARIABLES: \"TRUE\" ENV_VARIABLE_PREFIX: \"CFG_\" CFG_DB_HOST: \"http://localhost:3306\" CFG_DB_NAME: \"IOTstack Minecraft\" CFG_DB_PASSWORD_FILE: \"/run/secrets/db_password\" restart: unless-stopped rcon: image: itzg/rcon ports: - \"4326:4326\" - \"4327:4327\" volumes: - \"./volumes/rcon_data:/opt/rcon-web-admin/db\" secrets: db_password: file: ./db_password Then create the service directory that the new instance will use to store persistant data: mkdir -p ./volumes/minecraft and mkdir -p ./volumes/rcon_data Obviously you will need to give correct folder names depending on the volumes you specify for your custom services. If your new service doesn't require persistant storage, then you can skip this step. Then simply run the ./menu.sh command, and rebuild the stack with what ever services you had before. Using the Mosquitto example above, the final docker-compose.yml file will look like: version: '3.6' services: mosquitto: ports: - 1996:1996 - 9001:9001 container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: '1883' volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl minecraft: image: itzg/minecraft-server ports: - 25565:25565 volumes: - ./volumes/minecraft:/data environment: EULA: 'TRUE' TYPE: PAPER ENABLE_RCON: 'true' RCON_PASSWORD: PASSWORD RCON_PORT: 28016 VERSION: 1.15.2 REPLACE_ENV_VARIABLES: 'TRUE' ENV_VARIABLE_PREFIX: CFG_ CFG_DB_HOST: http://localhost:3306 CFG_DB_NAME: IOTstack Minecraft CFG_DB_PASSWORD_FILE: /run/secrets/db_password restart: unless-stopped rcon: image: itzg/rcon ports: - 4326:4326 - 4327:4327 volumes: - ./volumes/rcon_data:/opt/rcon-web-admin/db secrets: db_password: file: ./db_password Do note that the order of the YAML keys is not guaranteed.","title":"Custom services and overriding default settings for IOTstack"},{"location":"Custom/#custom-services-and-overriding-default-settings-for-iotstack","text":"You can specify modifcations to the docker-compose.yml file, including your own networks and custom containers/services. Create a file called compose-override.yml in the main directory, and place your modifications into it. These changes will be merged into the docker-compose.yml file next time you run the build script. The compose-override.yml file has been added to the .gitignore file, so it shouldn't be touched when upgrading IOTstack. It has been added to the backup script, and so will be included when you back up and restore IOTstack. Always test your backups though! New versions of IOTstack may break previous builds.","title":"Custom services and overriding default settings for IOTstack"},{"location":"Custom/#how-it-works","text":"After the build process has been completed, a temporary docker compose file is created in the tmp directory. The script then checks if compose-override.yml exists: If it exists, then continue to step 3 If it does not exist, copy the temporary docker compose file to the main directory and rename it to docker-compose.yml . Using the yaml_merge.py script, merge both the compose-override.yml and the temporary docker compose file together; Using the temporary file as the default values and interating through each level of the yaml structure, check to see if the compose-override.yml has a value set. Output the final file to the main directory, calling it docker-compose.yml .","title":"How it works"},{"location":"Custom/#a-word-of-caution","text":"If you specify an override for a service, and then rebuild the docker-compose.yml file, but deselect the service from the list, then the YAML merging will still produce that override. For example, lets say NodeRed was selected to have have the following override specified in compose-override.yml : services: nodered: restart: always When rebuilding the menu, ensure to have NodeRed service always included because if it's no longer included, the only values showing in the final docker-compose.yml file for NodeRed will be the restart key and its value. Docker Compose will error with the following message: Service nodered has neither an image nor a build context specified. At least one must be provided. When attempting to bring the services up with docker-compose up -d . Either remove the override for NodeRed in compose-override.yml and rebuild the stack, or ensure that NodeRed is built with the stack to fix this.","title":"A word of caution"},{"location":"Custom/#examples","text":"","title":"Examples"},{"location":"Custom/#overriding-default-settings","text":"Lets assume you put the following into the compose-override.yml file: services: mosquitto: ports: - 1996:1996 - 9001:9001 Normally the mosquitto service would be built like this inside the docker-compose.yml file: version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1883:1883 - 9001:9001 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Take special note of the ports list. If you run the build script with the compose-override.yml file in place, and open up the final docker-compose.yml file, you will notice that the port list have been replaced with the ones you specified in the compose-override.yml file. version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1996:1996 - 9001:9001 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl Do note that it will replace the entire list, if you were to specify services: mosquitto: ports: - 1996:1996 Then the final output will be: version: '3.6' services: mosquitto: container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: \"1883\" ports: - 1996:1996 volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./volumes/mosquitto/pwfile:/mosquitto/pwfile - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl","title":"Overriding default settings"},{"location":"Custom/#adding-custom-services","text":"Custom services can be added in a similar way to overriding default settings for standard services. Lets add a Minecraft and rcon server to IOTstack. Firstly, put the following into compose-override.yml : services: mosquitto: ports: - 1996:1996 - 9001:9001 minecraft: image: itzg/minecraft-server ports: - \"25565:25565\" volumes: - \"./volumes/minecraft:/data\" environment: EULA: \"TRUE\" TYPE: \"PAPER\" ENABLE_RCON: \"true\" RCON_PASSWORD: \"PASSWORD\" RCON_PORT: 28016 VERSION: \"1.15.2\" REPLACE_ENV_VARIABLES: \"TRUE\" ENV_VARIABLE_PREFIX: \"CFG_\" CFG_DB_HOST: \"http://localhost:3306\" CFG_DB_NAME: \"IOTstack Minecraft\" CFG_DB_PASSWORD_FILE: \"/run/secrets/db_password\" restart: unless-stopped rcon: image: itzg/rcon ports: - \"4326:4326\" - \"4327:4327\" volumes: - \"./volumes/rcon_data:/opt/rcon-web-admin/db\" secrets: db_password: file: ./db_password Then create the service directory that the new instance will use to store persistant data: mkdir -p ./volumes/minecraft and mkdir -p ./volumes/rcon_data Obviously you will need to give correct folder names depending on the volumes you specify for your custom services. If your new service doesn't require persistant storage, then you can skip this step. Then simply run the ./menu.sh command, and rebuild the stack with what ever services you had before. Using the Mosquitto example above, the final docker-compose.yml file will look like: version: '3.6' services: mosquitto: ports: - 1996:1996 - 9001:9001 container_name: mosquitto image: eclipse-mosquitto restart: unless-stopped user: '1883' volumes: - ./volumes/mosquitto/data:/mosquitto/data - ./volumes/mosquitto/log:/mosquitto/log - ./services/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf - ./services/mosquitto/filter.acl:/mosquitto/config/filter.acl minecraft: image: itzg/minecraft-server ports: - 25565:25565 volumes: - ./volumes/minecraft:/data environment: EULA: 'TRUE' TYPE: PAPER ENABLE_RCON: 'true' RCON_PASSWORD: PASSWORD RCON_PORT: 28016 VERSION: 1.15.2 REPLACE_ENV_VARIABLES: 'TRUE' ENV_VARIABLE_PREFIX: CFG_ CFG_DB_HOST: http://localhost:3306 CFG_DB_NAME: IOTstack Minecraft CFG_DB_PASSWORD_FILE: /run/secrets/db_password restart: unless-stopped rcon: image: itzg/rcon ports: - 4326:4326 - 4327:4327 volumes: - ./volumes/rcon_data:/opt/rcon-web-admin/db secrets: db_password: file: ./db_password Do note that the order of the YAML keys is not guaranteed.","title":"Adding custom services"},{"location":"Docker-commands/","text":"Docker commands Aliases I've added bash aliases for stopping and starting the stack. They can be installed in the docker commands menu. These commands no longer need to be executed from the IOTstack directory and can be executed in any directory alias iotstack_up=\"docker-compose -f ~/IOTstack/docker-compose.yml up -d\" alias iotstack_down=\"docker-compose -f ~/IOTstack/docker-compose.yml down\" alias iotstack_start=\"docker-compose -f ~/IOTstack/docker-compose.yml start\" alias iotstack_stop=\"docker-compose -f ~/IOTstack/docker-compose.yml stop\" alias iotstack_update=\"docker-compose -f ~/IOTstack/docker-compose.yml pull\" alias iotstack_build=\"docker-compose -f ~/IOTstack/docker-compose.yml build\" You can now type iotstack_up , they even accept additional parameters iotstack_stop portainer","title":"Docker commands"},{"location":"Docker-commands/#docker-commands","text":"","title":"Docker commands"},{"location":"Docker-commands/#aliases","text":"I've added bash aliases for stopping and starting the stack. They can be installed in the docker commands menu. These commands no longer need to be executed from the IOTstack directory and can be executed in any directory alias iotstack_up=\"docker-compose -f ~/IOTstack/docker-compose.yml up -d\" alias iotstack_down=\"docker-compose -f ~/IOTstack/docker-compose.yml down\" alias iotstack_start=\"docker-compose -f ~/IOTstack/docker-compose.yml start\" alias iotstack_stop=\"docker-compose -f ~/IOTstack/docker-compose.yml stop\" alias iotstack_update=\"docker-compose -f ~/IOTstack/docker-compose.yml pull\" alias iotstack_build=\"docker-compose -f ~/IOTstack/docker-compose.yml build\" You can now type iotstack_up , they even accept additional parameters iotstack_stop portainer","title":"Aliases"},{"location":"Getting-Started/","text":"Getting started Download the project On the lite image you will need to install git first sudo apt-get install git Then download with git clone https://github.com/gcgarner/IOTstack.git ~/IOTstack Due to some script restraints, this project needs to be stored in ~/IOTstack To enter the directory run: cd ~/IOTstack The Menu I've added a menu to make things easier. It is good however to familiarise yourself with how things are installed. The menu can be used to install docker and then build the docker-compose.yml file necessary for starting the stack and it runs a few common commands. I do recommend you start to learn the docker and docker-compose commands if you plan using docker in the long run. I've added several helper scripts, have a look inside. Navigate to the project folder and run ./menu.sh Installing from the menu Select the first option and follow the prompts Build the docker-compose file docker-compose uses the docker-compose.yml file to configure all the services. Run through the menu to select the options you want to install. Docker commands This menu executes shell scripts in the root of the project. It is not necessary to run them from the menu. Open up the shell script files to see what is inside and what they do Miscellaneous commands Some helpful commands have been added like disabling swap Running Docker commands From this point on make sure you are executing the commands from inside the project folder. Docker-compose commands need to be run from the folder where the docker-compose.yml is located. If you want to move the folder make sure you move the whole project folder. Starting and Stopping containers to start the stack navigate to the project folder containing the docker-compose.yml file To start the stack run: docker-compose up -d or ./scripts/start.sh To stop: docker-compose stop stops without removing containers To remove the stack: docker-compose down stops containers, deletes them and removes the network The first time you run 'start' the stack docker will download all the images for the web. Depending on how many containers you selected and your internet speed this can take a long while. Persistent data Docker allows you to map folders inside your containers to folders on the disk. This is done with the \"volume\" key. There are two types of volumes. Any modification to the container reflects in the volume. Sharing files between the Pi and containers Have a look a the wiki on how to share files between Node-RED and the Pi. Wiki Updating the images If a new version of a container image is available on docker hub it can be updated by a pull command. Use the docker-compose stop command to stop the stack Pull the latest version from docker hub with one of the following command docker-compose pull or the script ./scripts/update.sh Start the new stack based on the updated images docker-compose up -d Node-RED error after modifications to setup files The Node-RED image differs from the rest of the images in this project. It uses the \"build\" key. It uses a dockerfile for the setup to inject the nodes for pre-installation. If you get an error for Node-RED run docker-compose build then docker-compose up -d Deleting containers, volumes and images ./prune-images.sh will remove all images not associated with a container. If you run it while the stack is up it will ignore any in-use images. If you run this while you stack is down it will delete all images and you will have to redownload all images from scratch. This command can be helpful to reclaim disk space after updating your images, just make sure to run it while your stack is running as not to delete the images in use. (your data will still be safe in your volume mapping) Deleting folder volumes If you want to delete the influxdb data folder run the following command sudo rm -r volumes/influxdb/ . Only the data folder is deleted leaving the env file intact. review the docker-compose.yml file to see where the file volumes are stored. You can use git to delete all files and folders to return your folder to the freshly cloned state, AS IN YOU WILL LOSE ALL YOUR DATA. sudo git clean -d -x -f will return the working tree to its clean state. USE WITH CAUTION!","title":"Getting started"},{"location":"Getting-Started/#getting-started","text":"","title":"Getting started"},{"location":"Getting-Started/#download-the-project","text":"On the lite image you will need to install git first sudo apt-get install git Then download with git clone https://github.com/gcgarner/IOTstack.git ~/IOTstack Due to some script restraints, this project needs to be stored in ~/IOTstack To enter the directory run: cd ~/IOTstack","title":"Download the project"},{"location":"Getting-Started/#the-menu","text":"I've added a menu to make things easier. It is good however to familiarise yourself with how things are installed. The menu can be used to install docker and then build the docker-compose.yml file necessary for starting the stack and it runs a few common commands. I do recommend you start to learn the docker and docker-compose commands if you plan using docker in the long run. I've added several helper scripts, have a look inside. Navigate to the project folder and run ./menu.sh","title":"The Menu"},{"location":"Getting-Started/#installing-from-the-menu","text":"Select the first option and follow the prompts","title":"Installing from the menu"},{"location":"Getting-Started/#build-the-docker-compose-file","text":"docker-compose uses the docker-compose.yml file to configure all the services. Run through the menu to select the options you want to install.","title":"Build the docker-compose file"},{"location":"Getting-Started/#docker-commands","text":"This menu executes shell scripts in the root of the project. It is not necessary to run them from the menu. Open up the shell script files to see what is inside and what they do","title":"Docker commands"},{"location":"Getting-Started/#miscellaneous-commands","text":"Some helpful commands have been added like disabling swap","title":"Miscellaneous commands"},{"location":"Getting-Started/#running-docker-commands","text":"From this point on make sure you are executing the commands from inside the project folder. Docker-compose commands need to be run from the folder where the docker-compose.yml is located. If you want to move the folder make sure you move the whole project folder.","title":"Running Docker commands"},{"location":"Getting-Started/#starting-and-stopping-containers","text":"to start the stack navigate to the project folder containing the docker-compose.yml file To start the stack run: docker-compose up -d or ./scripts/start.sh To stop: docker-compose stop stops without removing containers To remove the stack: docker-compose down stops containers, deletes them and removes the network The first time you run 'start' the stack docker will download all the images for the web. Depending on how many containers you selected and your internet speed this can take a long while.","title":"Starting and Stopping containers"},{"location":"Getting-Started/#persistent-data","text":"Docker allows you to map folders inside your containers to folders on the disk. This is done with the \"volume\" key. There are two types of volumes. Any modification to the container reflects in the volume.","title":"Persistent data"},{"location":"Getting-Started/#sharing-files-between-the-pi-and-containers","text":"Have a look a the wiki on how to share files between Node-RED and the Pi. Wiki","title":"Sharing files between the Pi and containers"},{"location":"Getting-Started/#updating-the-images","text":"If a new version of a container image is available on docker hub it can be updated by a pull command. Use the docker-compose stop command to stop the stack Pull the latest version from docker hub with one of the following command docker-compose pull or the script ./scripts/update.sh Start the new stack based on the updated images docker-compose up -d","title":"Updating the images"},{"location":"Getting-Started/#node-red-error-after-modifications-to-setup-files","text":"The Node-RED image differs from the rest of the images in this project. It uses the \"build\" key. It uses a dockerfile for the setup to inject the nodes for pre-installation. If you get an error for Node-RED run docker-compose build then docker-compose up -d","title":"Node-RED error after modifications to setup files"},{"location":"Getting-Started/#deleting-containers-volumes-and-images","text":"./prune-images.sh will remove all images not associated with a container. If you run it while the stack is up it will ignore any in-use images. If you run this while you stack is down it will delete all images and you will have to redownload all images from scratch. This command can be helpful to reclaim disk space after updating your images, just make sure to run it while your stack is running as not to delete the images in use. (your data will still be safe in your volume mapping)","title":"Deleting containers, volumes and images"},{"location":"Getting-Started/#deleting-folder-volumes","text":"If you want to delete the influxdb data folder run the following command sudo rm -r volumes/influxdb/ . Only the data folder is deleted leaving the env file intact. review the docker-compose.yml file to see where the file volumes are stored. You can use git to delete all files and folders to return your folder to the freshly cloned state, AS IN YOU WILL LOSE ALL YOUR DATA. sudo git clean -d -x -f will return the working tree to its clean state. USE WITH CAUTION!","title":"Deleting folder volumes"},{"location":"Home/","text":"Wiki The README is moving to the Wiki, It's easier to add content and example to the Wiki vs the README.md Getting Started Updating the project How the script works Understanding Containers Docker Commands Docker Networks Containers Portainer Portainer Agent Node-RED Grafana Mosquitto PostgreSQL Adminer openHAB Home Assistant Pi-Hole zigbee2MQTT Plex TasmoAdmin RTL_433 EspruinoHub (testing) Next-Cloud MariaDB MotionEye Blynk Server diyHue Python Custom containers Native installs RTL_433 RPIEasy Backups Docker backups Recovery (coming soon) Remote Access VPN and Dynamic DNS x2go Miscellaneous log2ram Dropbox-Uploader","title":"Wiki"},{"location":"Home/#wiki","text":"The README is moving to the Wiki, It's easier to add content and example to the Wiki vs the README.md Getting Started Updating the project How the script works Understanding Containers","title":"Wiki"},{"location":"Home/#docker","text":"Commands Docker Networks","title":"Docker"},{"location":"Home/#containers","text":"Portainer Portainer Agent Node-RED Grafana Mosquitto PostgreSQL Adminer openHAB Home Assistant Pi-Hole zigbee2MQTT Plex TasmoAdmin RTL_433 EspruinoHub (testing) Next-Cloud MariaDB MotionEye Blynk Server diyHue Python Custom containers","title":"Containers"},{"location":"Home/#native-installs","text":"RTL_433 RPIEasy","title":"Native installs"},{"location":"Home/#backups","text":"Docker backups Recovery (coming soon)","title":"Backups"},{"location":"Home/#remote-access","text":"VPN and Dynamic DNS x2go","title":"Remote Access"},{"location":"Home/#miscellaneous","text":"log2ram Dropbox-Uploader","title":"Miscellaneous"},{"location":"How-the-script-works/","text":"How the script works The build script creates the ./services directory and populates it from the template file in .templates . The script then appends the text withing each service.yml file to the docker-compose.yml . When the stack is rebuild the menu doesn not overwrite the service folder if it already exists. Make sure to sync any alterations you have made to the docker-compose.yml file with the respective service.yml so that on your next build your changes pull through. The .gitignore file is setup such that if you do a git pull origin master it does not overwrite the files you have already created. Because the build script does not overwrite your service directory any changes in the .templates directory will have no affect on the services you have already made. You will need to move your service folder out to get the latest version of the template.","title":"How the script works"},{"location":"How-the-script-works/#how-the-script-works","text":"The build script creates the ./services directory and populates it from the template file in .templates . The script then appends the text withing each service.yml file to the docker-compose.yml . When the stack is rebuild the menu doesn not overwrite the service folder if it already exists. Make sure to sync any alterations you have made to the docker-compose.yml file with the respective service.yml so that on your next build your changes pull through. The .gitignore file is setup such that if you do a git pull origin master it does not overwrite the files you have already created. Because the build script does not overwrite your service directory any changes in the .templates directory will have no affect on the services you have already made. You will need to move your service folder out to get the latest version of the template.","title":"How the script works"},{"location":"Misc/","text":"Miscellaneous log2ram https://github.com/azlux/log2ram One of the drawbacks of an sd card is that it has a limited lifespan. One way to reduce the load on the sd card is to move your log files to RAM. log2ram is a convenient tool to simply set this up. It can be installed from the miscellaneous menu. Dropbox-Uploader This a great utility to easily upload data from your PI to the cloud. https://magpi.raspberrypi.org/articles/dropbox-raspberry-pi The MagPi has an excellent explanation of the process of setting up the Dropbox API. Dropbox-Uploader is used in the backup script.","title":"Miscellaneous"},{"location":"Misc/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"Misc/#log2ram","text":"https://github.com/azlux/log2ram One of the drawbacks of an sd card is that it has a limited lifespan. One way to reduce the load on the sd card is to move your log files to RAM. log2ram is a convenient tool to simply set this up. It can be installed from the miscellaneous menu.","title":"log2ram"},{"location":"Misc/#dropbox-uploader","text":"This a great utility to easily upload data from your PI to the cloud. https://magpi.raspberrypi.org/articles/dropbox-raspberry-pi The MagPi has an excellent explanation of the process of setting up the Dropbox API. Dropbox-Uploader is used in the backup script.","title":"Dropbox-Uploader"},{"location":"Native-RTL_433/","text":"Native RTL_433 RTL_433 can be installed from the \"Native install sections\" This video demonstrates how to use RTL_433","title":"Native RTL_433"},{"location":"Native-RTL_433/#native-rtl_433","text":"RTL_433 can be installed from the \"Native install sections\" This video demonstrates how to use RTL_433","title":"Native RTL_433"},{"location":"Networking/","text":"Networking The docker-compose instruction creates an internal network for the containers to communicate in, the ports get exposed to the PI's IP address when you want to connect from outside. It also creates a \"DNS\" the name being the container name. So it is important to note that when one container talks to another they talk by name. All the containers names are lowercase like nodered, influxdb... An easy way to find out your IP is by typing ip address in the terminal and look next to eth0 or wlan0 for your IP. It is highly recommended that you set a static IP for your PI or at least reserve an IP on your router so that you know it Check the docker-compose.yml to see which ports have been used Examples You want to connect your nodered to your mqtt server. In nodered drop an mqtt node, when you need to specify the address type mosquitto You want to connect to your influxdb from grafana. You are in the Docker network and you need to use the name of the Container. The address you specify in the grafana is http://influxdb:8086 You want to connect to the web interface of grafana from your laptop. Now you are outside the container environment you type PI's IP eg 192.168.n.m:3000 Ports Many containers try to use popular ports such as 80,443,8080. For example openHAB and Adminer both want to use port 8080 for their web interface. Adminer's port has been moved 9080 to accommodate this. Please check the description of the container in the README to see if there are any changes as they may not be the same as the port you are used to. Port mapping is done in the docker-compose.yml file. Each service should have a section that reads like this: ports: - HOST_PORT:CONTAINER_PORT For adminer: ports: - 9080:8080 Port 9080 on Host Pi is mapped to port 8080 of the container. Therefore 127.0.0.1:8080 will take you to openHAB, where 127.0.0.1:9080 will take you to adminer","title":"Networking"},{"location":"Networking/#networking","text":"The docker-compose instruction creates an internal network for the containers to communicate in, the ports get exposed to the PI's IP address when you want to connect from outside. It also creates a \"DNS\" the name being the container name. So it is important to note that when one container talks to another they talk by name. All the containers names are lowercase like nodered, influxdb... An easy way to find out your IP is by typing ip address in the terminal and look next to eth0 or wlan0 for your IP. It is highly recommended that you set a static IP for your PI or at least reserve an IP on your router so that you know it Check the docker-compose.yml to see which ports have been used","title":"Networking"},{"location":"Networking/#examples","text":"You want to connect your nodered to your mqtt server. In nodered drop an mqtt node, when you need to specify the address type mosquitto You want to connect to your influxdb from grafana. You are in the Docker network and you need to use the name of the Container. The address you specify in the grafana is http://influxdb:8086 You want to connect to the web interface of grafana from your laptop. Now you are outside the container environment you type PI's IP eg 192.168.n.m:3000","title":"Examples"},{"location":"Networking/#ports","text":"Many containers try to use popular ports such as 80,443,8080. For example openHAB and Adminer both want to use port 8080 for their web interface. Adminer's port has been moved 9080 to accommodate this. Please check the description of the container in the README to see if there are any changes as they may not be the same as the port you are used to. Port mapping is done in the docker-compose.yml file. Each service should have a section that reads like this: ports: - HOST_PORT:CONTAINER_PORT For adminer: ports: - 9080:8080 Port 9080 on Host Pi is mapped to port 8080 of the container. Therefore 127.0.0.1:8080 will take you to openHAB, where 127.0.0.1:9080 will take you to adminer","title":"Ports"},{"location":"RPIEasy_native/","text":"RPIEasy RPIEasy can now be installed under the native menu The installer will install any dependencies. If ~/rpieasy exists it will update the project to its latest, if not it will clone the project Running Running RPIEasy RPIEasy can be run by sudo ~/rpieasy/RPIEasy.py To have RPIEasy start on boot in the webui under hardware look for \"RPIEasy autostart at boot\" Ports RPIEasy will select its ports from the first available one in the list (80,8080,8008). If you run Hass.io then there will be a conflict so check the next available port","title":"RPIEasy"},{"location":"RPIEasy_native/#rpieasy","text":"RPIEasy can now be installed under the native menu The installer will install any dependencies. If ~/rpieasy exists it will update the project to its latest, if not it will clone the project","title":"RPIEasy"},{"location":"RPIEasy_native/#running-running-rpieasy","text":"RPIEasy can be run by sudo ~/rpieasy/RPIEasy.py To have RPIEasy start on boot in the webui under hardware look for \"RPIEasy autostart at boot\"","title":"Running Running RPIEasy"},{"location":"RPIEasy_native/#ports","text":"RPIEasy will select its ports from the first available one in the list (80,8080,8008). If you run Hass.io then there will be a conflict so check the next available port","title":"Ports"},{"location":"Understanding-Containers/","text":"Let\u2019s begin by understanding, What is Docker? In simple terms, Docker is a software platform that simplifies the process of building, running, managing and distributing applications. It does this by virtualizing the operating system of the computer on which it is installed and running. The Problem Let\u2019s say you have three different Python-based applications that you plan to host on a single server (which could either be a physical or a virtual machine). Each of these applications makes use of a different version of Python, as well as the associated libraries and dependencies, differ from one application to another. Since we cannot have different versions of Python installed on the same machine, this prevents us from hosting all three applications on the same computer. The Solution Let\u2019s look at how we could solve this problem without making use of Docker. In such a scenario, we could solve this problem either by having three physical machines, or a single physical machine, which is powerful enough to host and run three virtual machines on it. Both the options would allow us to install different versions of Python on each of these machines, along with their associated dependencies. The machine on which Docker is installed and running is usually referred to as a Docker Host or Host in simple terms. So, whenever you plan to deploy an application on the host, it would create a logical entity on it to host that application. In Docker terminology, we call this logical entity a Container or Docker Container to be more precise. Whereas the kernel of the host\u2019s operating system is shared across all the containers that are running on it. This allows each container to be isolated from the other present on the same host. Thus it supports multiple containers with different application requirements and dependencies to run on the same host, as long as they have the same operating system requirements. Docker Terminology Docker Images and Docker Containers are the two essential things that you will come across daily while working with Docker. In simple terms, a Docker Image is a template that contains the application, and all the dependencies required to run that application on Docker. On the other hand, as stated earlier, a Docker Container is a logical entity. In more precise terms, it is a running instance of the Docker Image. What is Docker-Compose? Docker Compose provides a way to orchestrate multiple containers that work together. Docker compose is a simple yet powerful tool that is used to run multiple containers as a single service. For example, suppose you have an application which requires Mqtt as a communication service between IOT devices and OpenHAB instance as a Smarthome application service. In this case by docker-compose, you can create one single file (docker-compose.yml) which will create both the containers as a single service without starting each separately. It wires up the networks (literally), mounts all volumes and exposes the ports. The IOTstack with the templates and menu is a generator for that docker-compose service descriptor. How Docker Compose Works? use yaml files to configure application services (docker-compose.yaml) can start all the services with a single command ( docker-compose up ) can stop all the service with a single command ( docker-compose down ) How are the containers connected The containers are automagically connected when we run the stack with docker-compose up. The containers using same logical network (by default) where the instances can access each other with the instance logical name. Means if there is an instance called mosquitto and an openhab , when openHAB instance need to access mqtt on that case the domain name of mosquitto will be resolved as the runnuning instance of mosquitto. How the container are connected to host machine Volumes The containers are enclosed processes which state are lost with the restart of container. To be able to persist states volumes (images or directories) can be used to share data with the host. Which means if you need to persist some database, configuration or any state you have to bind volumes where the running service inside the container will write files to that binded volume. In order to understand what a Docker volume is, we first need to be clear about how the filesystem normally works in Docker. Docker images are stored as series of read-only layers. When we start a container, Docker takes the read-only image and adds a read-write layer on top. If the running container modifies an existing file, the file is copied out of the underlying read-only layer and into the top-most read-write layer where the changes are applied. The version in the read-write layer hides the underlying file, but does not destroy it -- it still exists in the underlying layer. When a Docker container is deleted, relaunching the image will start a fresh container without any of the changes made in the previously running container -- those changes are lost, thats the reason that configs, databases are not persisted, Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure of the host machine, volumes are completely managed by Docker. In IOTstack project uses the volumes directory in general to bind these container volumes. Ports When containers running a we would like to delegate some services to the outside world, for example OpenHAB web frontend have to be accessible for users. There are several ways to achive that. One is mounting the port to the most machine, this called port binding. On that case service will have a dedicated port which can be accessed, one drawback is one host port can be used one serice only. Another way is reverse proxy. The term reverse proxy (or Load Balancer in some terminology) is normally applied to a service that sits in front of one or more servers (in our case containers), accepting requests from clients for resources located on the server(s). From the client point of view, the reverse proxy appears to be the web server and so is totally transparent to the remote user. Which means several service can share same port the server will route the request by the URL (virtual domain or context path). For example, there is grafana and openHAB instances, where the opeanhab.domain.tld request will be routed to openHAB instance 8181 port while grafana.domain.tld to grafana instance 3000 port. On that case the proxy have to be mapped for host port 80 and/or 444 on host machine, the proxy server will access the containers via the docker virtual network. Source materials used: https://takacsmark.com/docker-compose-tutorial-beginners-by-example-basics/ https://www.freecodecamp.org/news/docker-simplified-96639a35ff36/ https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/ https://blog.container-solutions.com/understanding-volumes-docker","title":"Let\u2019s begin by understanding, What is Docker?"},{"location":"Understanding-Containers/#lets-begin-by-understanding-what-is-docker","text":"In simple terms, Docker is a software platform that simplifies the process of building, running, managing and distributing applications. It does this by virtualizing the operating system of the computer on which it is installed and running.","title":"Let\u2019s begin by understanding, What is Docker?"},{"location":"Understanding-Containers/#the-problem","text":"Let\u2019s say you have three different Python-based applications that you plan to host on a single server (which could either be a physical or a virtual machine). Each of these applications makes use of a different version of Python, as well as the associated libraries and dependencies, differ from one application to another. Since we cannot have different versions of Python installed on the same machine, this prevents us from hosting all three applications on the same computer.","title":"The Problem"},{"location":"Understanding-Containers/#the-solution","text":"Let\u2019s look at how we could solve this problem without making use of Docker. In such a scenario, we could solve this problem either by having three physical machines, or a single physical machine, which is powerful enough to host and run three virtual machines on it. Both the options would allow us to install different versions of Python on each of these machines, along with their associated dependencies. The machine on which Docker is installed and running is usually referred to as a Docker Host or Host in simple terms. So, whenever you plan to deploy an application on the host, it would create a logical entity on it to host that application. In Docker terminology, we call this logical entity a Container or Docker Container to be more precise. Whereas the kernel of the host\u2019s operating system is shared across all the containers that are running on it. This allows each container to be isolated from the other present on the same host. Thus it supports multiple containers with different application requirements and dependencies to run on the same host, as long as they have the same operating system requirements.","title":"The Solution"},{"location":"Understanding-Containers/#docker-terminology","text":"Docker Images and Docker Containers are the two essential things that you will come across daily while working with Docker. In simple terms, a Docker Image is a template that contains the application, and all the dependencies required to run that application on Docker. On the other hand, as stated earlier, a Docker Container is a logical entity. In more precise terms, it is a running instance of the Docker Image.","title":"Docker Terminology"},{"location":"Understanding-Containers/#what-is-docker-compose","text":"Docker Compose provides a way to orchestrate multiple containers that work together. Docker compose is a simple yet powerful tool that is used to run multiple containers as a single service. For example, suppose you have an application which requires Mqtt as a communication service between IOT devices and OpenHAB instance as a Smarthome application service. In this case by docker-compose, you can create one single file (docker-compose.yml) which will create both the containers as a single service without starting each separately. It wires up the networks (literally), mounts all volumes and exposes the ports. The IOTstack with the templates and menu is a generator for that docker-compose service descriptor.","title":"What is Docker-Compose?"},{"location":"Understanding-Containers/#how-docker-compose-works","text":"use yaml files to configure application services (docker-compose.yaml) can start all the services with a single command ( docker-compose up ) can stop all the service with a single command ( docker-compose down )","title":"How Docker Compose Works?"},{"location":"Understanding-Containers/#how-are-the-containers-connected","text":"The containers are automagically connected when we run the stack with docker-compose up. The containers using same logical network (by default) where the instances can access each other with the instance logical name. Means if there is an instance called mosquitto and an openhab , when openHAB instance need to access mqtt on that case the domain name of mosquitto will be resolved as the runnuning instance of mosquitto.","title":"How are the containers connected"},{"location":"Understanding-Containers/#how-the-container-are-connected-to-host-machine","text":"","title":"How the container are connected to host machine"},{"location":"Understanding-Containers/#volumes","text":"The containers are enclosed processes which state are lost with the restart of container. To be able to persist states volumes (images or directories) can be used to share data with the host. Which means if you need to persist some database, configuration or any state you have to bind volumes where the running service inside the container will write files to that binded volume. In order to understand what a Docker volume is, we first need to be clear about how the filesystem normally works in Docker. Docker images are stored as series of read-only layers. When we start a container, Docker takes the read-only image and adds a read-write layer on top. If the running container modifies an existing file, the file is copied out of the underlying read-only layer and into the top-most read-write layer where the changes are applied. The version in the read-write layer hides the underlying file, but does not destroy it -- it still exists in the underlying layer. When a Docker container is deleted, relaunching the image will start a fresh container without any of the changes made in the previously running container -- those changes are lost, thats the reason that configs, databases are not persisted, Volumes are the preferred mechanism for persisting data generated by and used by Docker containers. While bind mounts are dependent on the directory structure of the host machine, volumes are completely managed by Docker. In IOTstack project uses the volumes directory in general to bind these container volumes.","title":"Volumes"},{"location":"Understanding-Containers/#ports","text":"When containers running a we would like to delegate some services to the outside world, for example OpenHAB web frontend have to be accessible for users. There are several ways to achive that. One is mounting the port to the most machine, this called port binding. On that case service will have a dedicated port which can be accessed, one drawback is one host port can be used one serice only. Another way is reverse proxy. The term reverse proxy (or Load Balancer in some terminology) is normally applied to a service that sits in front of one or more servers (in our case containers), accepting requests from clients for resources located on the server(s). From the client point of view, the reverse proxy appears to be the web server and so is totally transparent to the remote user. Which means several service can share same port the server will route the request by the URL (virtual domain or context path). For example, there is grafana and openHAB instances, where the opeanhab.domain.tld request will be routed to openHAB instance 8181 port while grafana.domain.tld to grafana instance 3000 port. On that case the proxy have to be mapped for host port 80 and/or 444 on host machine, the proxy server will access the containers via the docker virtual network. Source materials used: https://takacsmark.com/docker-compose-tutorial-beginners-by-example-basics/ https://www.freecodecamp.org/news/docker-simplified-96639a35ff36/ https://www.cloudflare.com/learning/cdn/glossary/reverse-proxy/ https://blog.container-solutions.com/understanding-volumes-docker","title":"Ports"},{"location":"Updating-the-Project/","text":"Updating the project If you ran the git checkout -- 'git ls-files -m' as suggested in the old wiki entry then please check your duck.sh because it removed your domain and token Periodically updates are made to project which include new or modified container template, changes to backups or additional features. As these are released your local copy of this project will become out of date. This section deals with how to bring your project to the latest published state. Git offers build in functionality to fetch the latest changes. git pull origin master will fetch the latest changes from GitHub without overwriting files that you have modified yourself. If you have done a local commit then your project may to handle a merge conflict. This can be verified by running git status . You can ignore if it reports duck.sh as being modified. Should you have any modified scripts or templates they can be reset to the latest version with git checkout -- scripts/ .templates/ With the new latest version of the project you can now use the menu to build your stack. If there is a particular container you would like to update its template then you can select that at the overwrite option for your container. You have the choice to not to overwrite, preserve env files or to completely overwrite any changes (passwords) After your stack had been rebuild you can run docker-compose up -d to pull in the latest changes. If you have not update your images in a while consider running the ./scripts/update.sh to get the latest version of the image from Docker hub as well","title":"Updating the project"},{"location":"Updating-the-Project/#updating-the-project","text":"If you ran the git checkout -- 'git ls-files -m' as suggested in the old wiki entry then please check your duck.sh because it removed your domain and token Periodically updates are made to project which include new or modified container template, changes to backups or additional features. As these are released your local copy of this project will become out of date. This section deals with how to bring your project to the latest published state. Git offers build in functionality to fetch the latest changes. git pull origin master will fetch the latest changes from GitHub without overwriting files that you have modified yourself. If you have done a local commit then your project may to handle a merge conflict. This can be verified by running git status . You can ignore if it reports duck.sh as being modified. Should you have any modified scripts or templates they can be reset to the latest version with git checkout -- scripts/ .templates/ With the new latest version of the project you can now use the menu to build your stack. If there is a particular container you would like to update its template then you can select that at the overwrite option for your container. You have the choice to not to overwrite, preserve env files or to completely overwrite any changes (passwords) After your stack had been rebuild you can run docker-compose up -d to pull in the latest changes. If you have not update your images in a while consider running the ./scripts/update.sh to get the latest version of the image from Docker hub as well","title":"Updating the project"},{"location":"Containers/Adminer/","text":"Adminer References Docker Website About This is a nice tool for managing databases. Web interface has moved to port 9080. There was an issue where openHAB and Adminer were using the same ports. If you have an port conflict edit the docker-compose.yml and under the adminer service change the line to read: ports: - 9080:8080","title":"Adminer"},{"location":"Containers/Adminer/#adminer","text":"","title":"Adminer"},{"location":"Containers/Adminer/#references","text":"Docker Website","title":"References"},{"location":"Containers/Adminer/#about","text":"This is a nice tool for managing databases. Web interface has moved to port 9080. There was an issue where openHAB and Adminer were using the same ports. If you have an port conflict edit the docker-compose.yml and under the adminer service change the line to read: ports: - 9080:8080","title":"About"},{"location":"Containers/Blynk_server/","text":"Blynk server This is a custom implementation of Blynk Server blynk_server: build: ./services/blynk_server/. container_name: blynk_server restart: unless-stopped ports: - 8180:8080 - 8441:8441 - 9443:9443 volumes: - ./volumes/blynk_server/data:/data To connect to the admin interface navigate to <your pis IP>:9443/admin I don't know anything about this service so you will need to read though the setup on the Project Homepage When setting up the application on your mobile be sure to select custom setup here Writeup From @877dev Getting started Log into admin panel at https://youripaddress:9443/admin (Use your Pi's IP address, and ignore Chrome warning). Default credentials: user:admin@blynk.cc pass:admin Change username and password Click on Users > \"email address\" and edit email, name and password. Save changes Restarting the container using Portainer may be required to take effect. Setup gmail Optional step, useful for getting the auth token emailed to you. (To be added once confirmed working....) iOS/Android app setup Login the app as per the photos HERE Press \"New Project\" Give it a name, choose device \"Raspberry Pi 3 B\" so you have plenty of virtual pins available, and lastly select WiFi. Create project and the auth token will be emailed to you (if emails configured). You can also find the token in app under the phone app settings, or in the admin web interface by clicking Users>\"email address\" and scroll down to token. Quick usage guide for app Press on the empty page, the widgets will appear from the right. Select your widget, let's say a button. It appears on the page, press on it to configure. Give it a name and colour if you want. Press on PIN, and select virtual. Choose any pin i.e. V0 Press ok. To start the project running, press top right Play button. You will get an offline message, because no devices are connected to your project via the token. Enter node red..... Node red Install node-red-contrib-blynk-ws from pallette manager Drag a \"write event\" node into your flow, and connect to a debug node Configure the Blynk node for the first time: URL: wss://youripaddress:9443/websockets more info HERE Enter your auth token from before and save/exit. When you deploy the flow, notice the app shows connected message, as does the Blynk node. Press the button on the app, you will notice the payload is sent to the debug node. What next? Further information and advanced setup: https://github.com/blynkkk/blynk-server Check the documentation: https://docs.blynk.cc/ Visit the community forum pages: https://community.blynk.cc/ Interesting post by Peter Knight on MQTT/Node Red flows: https://community.blynk.cc/t/my-home-automation-projects-built-with-mqtt-and-node-red/29045 Some Blynk flow examples: https://github.com/877dev/Node-Red-flow-examples","title":"Blynk server"},{"location":"Containers/Blynk_server/#blynk-server","text":"This is a custom implementation of Blynk Server blynk_server: build: ./services/blynk_server/. container_name: blynk_server restart: unless-stopped ports: - 8180:8080 - 8441:8441 - 9443:9443 volumes: - ./volumes/blynk_server/data:/data To connect to the admin interface navigate to <your pis IP>:9443/admin I don't know anything about this service so you will need to read though the setup on the Project Homepage When setting up the application on your mobile be sure to select custom setup here Writeup From @877dev","title":"Blynk server"},{"location":"Containers/Blynk_server/#getting-started","text":"Log into admin panel at https://youripaddress:9443/admin (Use your Pi's IP address, and ignore Chrome warning). Default credentials: user:admin@blynk.cc pass:admin","title":"Getting started"},{"location":"Containers/Blynk_server/#change-username-and-password","text":"Click on Users > \"email address\" and edit email, name and password. Save changes Restarting the container using Portainer may be required to take effect.","title":"Change username and password"},{"location":"Containers/Blynk_server/#setup-gmail","text":"Optional step, useful for getting the auth token emailed to you. (To be added once confirmed working....)","title":"Setup gmail"},{"location":"Containers/Blynk_server/#iosandroid-app-setup","text":"Login the app as per the photos HERE Press \"New Project\" Give it a name, choose device \"Raspberry Pi 3 B\" so you have plenty of virtual pins available, and lastly select WiFi. Create project and the auth token will be emailed to you (if emails configured). You can also find the token in app under the phone app settings, or in the admin web interface by clicking Users>\"email address\" and scroll down to token.","title":"iOS/Android app setup"},{"location":"Containers/Blynk_server/#quick-usage-guide-for-app","text":"Press on the empty page, the widgets will appear from the right. Select your widget, let's say a button. It appears on the page, press on it to configure. Give it a name and colour if you want. Press on PIN, and select virtual. Choose any pin i.e. V0 Press ok. To start the project running, press top right Play button. You will get an offline message, because no devices are connected to your project via the token. Enter node red.....","title":"Quick usage guide for app"},{"location":"Containers/Blynk_server/#node-red","text":"Install node-red-contrib-blynk-ws from pallette manager Drag a \"write event\" node into your flow, and connect to a debug node Configure the Blynk node for the first time: URL: wss://youripaddress:9443/websockets more info HERE Enter your auth token from before and save/exit. When you deploy the flow, notice the app shows connected message, as does the Blynk node. Press the button on the app, you will notice the payload is sent to the debug node.","title":"Node red"},{"location":"Containers/Blynk_server/#what-next","text":"Further information and advanced setup: https://github.com/blynkkk/blynk-server Check the documentation: https://docs.blynk.cc/ Visit the community forum pages: https://community.blynk.cc/ Interesting post by Peter Knight on MQTT/Node Red flows: https://community.blynk.cc/t/my-home-automation-projects-built-with-mqtt-and-node-red/29045 Some Blynk flow examples: https://github.com/877dev/Node-Red-flow-examples","title":"What next?"},{"location":"Containers/EspruinoHub/","text":"Espruinohub This is a testing container I tried it however the container keeps restarting docker logs espruinohub I get \"BLE Broken?\" but could just be i dont have any BLE devices nearby web interface is on \"{your_Pis_IP}:1888\" see https://github.com/espruino/EspruinoHub#status--websocket-mqtt--espruino-web-ide for other details there were no recommendations for persistent data volumes. so docker-compose down may destroy all you configurations so use docker-compose stop in stead please let me know about your success or issues here","title":"Espruinohub"},{"location":"Containers/EspruinoHub/#espruinohub","text":"This is a testing container I tried it however the container keeps restarting docker logs espruinohub I get \"BLE Broken?\" but could just be i dont have any BLE devices nearby web interface is on \"{your_Pis_IP}:1888\" see https://github.com/espruino/EspruinoHub#status--websocket-mqtt--espruino-web-ide for other details there were no recommendations for persistent data volumes. so docker-compose down may destroy all you configurations so use docker-compose stop in stead please let me know about your success or issues here","title":"Espruinohub"},{"location":"Containers/Grafana/","text":"Grafana References Docker Website Setting your time-zone The default ~/IOTstack/services/grafana/grafana.env contains this line: #TZ=Africa/Johannesburg Uncomment that line and change the right hand side to your own timezone . Security If Grafana has just been installed but has never been launched then the following will be true: The folder ~/IOTstack/volumes/grafana will not exist; and The file ~/IOTstack/services/grafana/grafana.env will contain these lines: ``` GF_SECURITY_ADMIN_USER=admin GF_SECURITY_ADMIN_PASSWORD=admin ``` You should see those lines as documentation rather than something you are being invited to edit. It is telling you that the default administative user for Grafana is \"admin\" and that the default password for that user is \"admin\". If you do not change anything then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will: Expect you to login as user \"admin\" with password \"admin\"; and then Force you to change the default password to something else. Thereafter, you will login as \"admin\" with whatever password you chose. You can change the administrator's password as often as you like via the web UI ( profile button, change password tab). This method (of not touching these two keys in grafana.env ) is the recommended approach. Please try to resist the temptation to fiddle! I want a different admin username (not recommended) If, before you bring up the stack for the first time, you do this: GF_SECURITY_ADMIN_USER=jack #GF_SECURITY_ADMIN_PASSWORD=admin then, when you bring up the stack and connect on port 3000, Grafana will: Expect you to login as user \"jack\" with password \"admin\"; and then Force you to change the default password to something else. Thereafter, \"jack\" will be the Grafana administrator and you will login with the password you chose, until you decide to change the password to something else via the web UI. Don't think you can come back later and tweak the Grafana administrator name in the environment variables. It doesn't work that way. It's a one-shot. I want a different default admin password (not recommended) Well, first off, the two methods above both make you set a different password on first login so there probably isn't much point to this. But, if you really insist\u2026 If, before you bring up the stack for the first time, you do this: #GF_SECURITY_ADMIN_USER=admin GF_SECURITY_ADMIN_PASSWORD=jack then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will expect you to login as user \"admin\" with password \"jack\". Grafana will not force you to change the password on first login but you will still be able to change it via the web UI. But don't think you can come back later and change the password in the environment variables. It doesn't work that way. It's a one-shot. I want to change everything (not recommended) If, before you bring up the stack for the first time, you do this: GF_SECURITY_ADMIN_USER=bill GF_SECURITY_ADMIN_PASSWORD=jack then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will expect you to login as user \"bill\" with password \"jack\". Grafana will not force you to change the password on first login but you will still be able to change it via the web UI. But don't think you can come back later and tweak either the username or password in the environment variables. It doesn't work that way. It's a one-shot. Distilling it down Before Grafana is launched for the first time: GF_SECURITY_ADMIN_USER has a default value of \"admin\". You can explicitly set it to \"admin\" or some other value. Whatever option you choose then that's the account name of Grafana's administrative user. But choosing any value other than \"admin\" is probably a bad idea. GF_SECURITY_ADMIN_PASSWORD has a default value of \"admin\". You can explicitly set it to \"admin\" or some other value. If its value is \"admin\" then you will be forced to change it the first time you login to Grafana. If its value is something other than \"admin\" then that will be the password until you change it via the web UI. These two environment keys only work for the first launch of Grafana. Once Grafana has been launched, you can never change either the username or the password by editing grafana.env . For this reason, it is better to leave grafana.env in its shrink-wrapped state. Your first login is as \"admin/admin\" and then you set the password you actually want when Grafana prompts you to change it. HELP \u2013 I forgot my Grafana admin password! Assuming your IOTstack is up, the magic incantation is: $ docker exec grafana grafana-cli --homepath \"/usr/share/grafana\" admin reset-admin-password \"admin\" Then, use a browser to connect to your Raspberry Pi on port 3000. Grafana will: Expect you login as user \"admin\" with password \"admin\"; and then Force you to change the default password to something else. This magic incantation assumes that your administrative username is \"admin\". If you ignored the advice above and changed the administator username to something else then all bets are off. It might work anyway but we haven't tested it. Sorry. But that's why we said changing the username was not recommended. Overriding Grafana settings Grafana documentation contains a list of settings . Settings are described in terms of how they appear in \".ini\" files. An example of the sort of thing you might want to do is to enable anonymous access to your Grafana dashboards. The Grafana documentation describes this in \".ini\" format as: [auth.anonymous] enabled = true # Organization name that should be used for unauthenticated users org_name = Main Org. # Role for unauthenticated users, other valid values are `Editor` and `Admin` org_role = Viewer \".ini\" format is not really appropriate in a Docker context. Instead, you use environment variables to override Docker's settings. Environment variables are placed in ~/IOTstack/services/grafana/grafana.env . You need to convert \".ini\" format to environment variable syntax. The rules are: Start with \"GF_\", then Append the [section name], replacing any periods with underscores, then Append the section key \"as is\", then Append an \"=\", then Append the right hand side in quotes. Applying those rules gets you: GF_AUTH_ANONYMOUS_ENABLED=\"true\" GF_AUTH_ANONYMOUS_ORG_NAME=\"Main Org.\" GF_AUTH_ANONYMOUS_ORG_ROLE=\"Viewer\" It is not strictly necessary to encapsulate every right hand side value in quotes. In the above, both \"true\" and \"Viewer\" would work without quotes, whereas \"Main Org.\" needs quotes because of the embedded space. After you have changed ~/IOTstack/services/grafana/grafana.env , you need to propagate the changes into the Grafana container: $ cd ~/IOTstack $ docker-compose stop grafana $ docker-compose up -d In theory, the second command could be omitted, or both the second and third commands could be replaced with \"docker-compose restart grafana\" but experience suggests stopping the container is more reliable. A slightly more real-world example would involve choosing a different default organisation name for anonymous access. This example uses \"ChezMoi\". First, the environment key needs to be set to that value: GF_AUTH_ANONYMOUS_ORG_NAME=ChezMoi Then that change needs to be propagated into the Grafana container as explained above. Next, Grafana needs to be told that \"ChezMoi\" is the default organisation: Use your browser to login to Grafana as an administrator. From the \"Server Admin\" slide-out menu on the left hand side, choose \"Orgs\". In the list that appears, click on \"Main Org\". This opens an editing panel. Change the \"Name\" field to \"ChezMoi\" and click \"Update\". Sign-out of Grafana. You will be taken back to the login screen. Your URL bar will look something like this: http://myhost.mydomain.com:3000/login 6. Edit the URL to remove the \"login\" suffix and press return. If all your changes were applied successfully, you will have anonymous access and the URL will look something like this: http://myhost.mydomain.com:3000/?orgId=1 HELP \u2013 I made a mess! \"I made a bit of a mess with Grafana. First time user. Steep learning curve. False starts, many. Mistakes, unavoidable. Been there, done that. But now I really need to start from a clean slate. And, yes, I understand there is no undo for this.\" Begin by stopping Grafana: $ cd ~/IOTstack $ docker-compose stop grafana You have two options: Destroy your settings and dashboards but retain any plugins you may have installed: $ sudo rm ~/IOTstack/volumes/grafana/data/grafana.db Nuke everything (triple-check this command before you hit return): $ sudo rm -rf ~/IOTstack/volumes/grafana/data This is where you should edit ~/IOTstack/services/grafana/grafana.env to correct any problems (such as choosing an administrative username other than \"admin\"). When you are ready, bring Grafana back up again: $ cd ~/IOTstack $ docker-compose up -d Grafana will automatically recreate everything it needs. You will be able to login as \"admin/admin\".","title":"Grafana"},{"location":"Containers/Grafana/#grafana","text":"","title":"Grafana"},{"location":"Containers/Grafana/#references","text":"Docker Website","title":"References"},{"location":"Containers/Grafana/#setting-your-time-zone","text":"The default ~/IOTstack/services/grafana/grafana.env contains this line: #TZ=Africa/Johannesburg Uncomment that line and change the right hand side to your own timezone .","title":"Setting your time-zone"},{"location":"Containers/Grafana/#security","text":"If Grafana has just been installed but has never been launched then the following will be true: The folder ~/IOTstack/volumes/grafana will not exist; and The file ~/IOTstack/services/grafana/grafana.env will contain these lines: ```","title":"Security"},{"location":"Containers/Grafana/#gf_security_admin_useradmin","text":"","title":"GF_SECURITY_ADMIN_USER=admin"},{"location":"Containers/Grafana/#gf_security_admin_passwordadmin","text":"``` You should see those lines as documentation rather than something you are being invited to edit. It is telling you that the default administative user for Grafana is \"admin\" and that the default password for that user is \"admin\". If you do not change anything then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will: Expect you to login as user \"admin\" with password \"admin\"; and then Force you to change the default password to something else. Thereafter, you will login as \"admin\" with whatever password you chose. You can change the administrator's password as often as you like via the web UI ( profile button, change password tab). This method (of not touching these two keys in grafana.env ) is the recommended approach. Please try to resist the temptation to fiddle!","title":"GF_SECURITY_ADMIN_PASSWORD=admin"},{"location":"Containers/Grafana/#i-want-a-different-admin-username-not-recommended","text":"If, before you bring up the stack for the first time, you do this: GF_SECURITY_ADMIN_USER=jack #GF_SECURITY_ADMIN_PASSWORD=admin then, when you bring up the stack and connect on port 3000, Grafana will: Expect you to login as user \"jack\" with password \"admin\"; and then Force you to change the default password to something else. Thereafter, \"jack\" will be the Grafana administrator and you will login with the password you chose, until you decide to change the password to something else via the web UI. Don't think you can come back later and tweak the Grafana administrator name in the environment variables. It doesn't work that way. It's a one-shot.","title":"I want a different admin username (not recommended)"},{"location":"Containers/Grafana/#i-want-a-different-default-admin-password-not-recommended","text":"Well, first off, the two methods above both make you set a different password on first login so there probably isn't much point to this. But, if you really insist\u2026 If, before you bring up the stack for the first time, you do this: #GF_SECURITY_ADMIN_USER=admin GF_SECURITY_ADMIN_PASSWORD=jack then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will expect you to login as user \"admin\" with password \"jack\". Grafana will not force you to change the password on first login but you will still be able to change it via the web UI. But don't think you can come back later and change the password in the environment variables. It doesn't work that way. It's a one-shot.","title":"I want a different default admin password (not recommended)"},{"location":"Containers/Grafana/#i-want-to-change-everything-not-recommended","text":"If, before you bring up the stack for the first time, you do this: GF_SECURITY_ADMIN_USER=bill GF_SECURITY_ADMIN_PASSWORD=jack then, when you bring up the stack and use a browser to connect to your Raspberry Pi on port 3000, Grafana will expect you to login as user \"bill\" with password \"jack\". Grafana will not force you to change the password on first login but you will still be able to change it via the web UI. But don't think you can come back later and tweak either the username or password in the environment variables. It doesn't work that way. It's a one-shot.","title":"I want to change everything (not recommended)"},{"location":"Containers/Grafana/#distilling-it-down","text":"Before Grafana is launched for the first time: GF_SECURITY_ADMIN_USER has a default value of \"admin\". You can explicitly set it to \"admin\" or some other value. Whatever option you choose then that's the account name of Grafana's administrative user. But choosing any value other than \"admin\" is probably a bad idea. GF_SECURITY_ADMIN_PASSWORD has a default value of \"admin\". You can explicitly set it to \"admin\" or some other value. If its value is \"admin\" then you will be forced to change it the first time you login to Grafana. If its value is something other than \"admin\" then that will be the password until you change it via the web UI. These two environment keys only work for the first launch of Grafana. Once Grafana has been launched, you can never change either the username or the password by editing grafana.env . For this reason, it is better to leave grafana.env in its shrink-wrapped state. Your first login is as \"admin/admin\" and then you set the password you actually want when Grafana prompts you to change it.","title":"Distilling it down"},{"location":"Containers/Grafana/#help-i-forgot-my-grafana-admin-password","text":"Assuming your IOTstack is up, the magic incantation is: $ docker exec grafana grafana-cli --homepath \"/usr/share/grafana\" admin reset-admin-password \"admin\" Then, use a browser to connect to your Raspberry Pi on port 3000. Grafana will: Expect you login as user \"admin\" with password \"admin\"; and then Force you to change the default password to something else. This magic incantation assumes that your administrative username is \"admin\". If you ignored the advice above and changed the administator username to something else then all bets are off. It might work anyway but we haven't tested it. Sorry. But that's why we said changing the username was not recommended.","title":"HELP \u2013 I forgot my Grafana admin password!"},{"location":"Containers/Grafana/#overriding-grafana-settings","text":"Grafana documentation contains a list of settings . Settings are described in terms of how they appear in \".ini\" files. An example of the sort of thing you might want to do is to enable anonymous access to your Grafana dashboards. The Grafana documentation describes this in \".ini\" format as: [auth.anonymous] enabled = true # Organization name that should be used for unauthenticated users org_name = Main Org. # Role for unauthenticated users, other valid values are `Editor` and `Admin` org_role = Viewer \".ini\" format is not really appropriate in a Docker context. Instead, you use environment variables to override Docker's settings. Environment variables are placed in ~/IOTstack/services/grafana/grafana.env . You need to convert \".ini\" format to environment variable syntax. The rules are: Start with \"GF_\", then Append the [section name], replacing any periods with underscores, then Append the section key \"as is\", then Append an \"=\", then Append the right hand side in quotes. Applying those rules gets you: GF_AUTH_ANONYMOUS_ENABLED=\"true\" GF_AUTH_ANONYMOUS_ORG_NAME=\"Main Org.\" GF_AUTH_ANONYMOUS_ORG_ROLE=\"Viewer\" It is not strictly necessary to encapsulate every right hand side value in quotes. In the above, both \"true\" and \"Viewer\" would work without quotes, whereas \"Main Org.\" needs quotes because of the embedded space. After you have changed ~/IOTstack/services/grafana/grafana.env , you need to propagate the changes into the Grafana container: $ cd ~/IOTstack $ docker-compose stop grafana $ docker-compose up -d In theory, the second command could be omitted, or both the second and third commands could be replaced with \"docker-compose restart grafana\" but experience suggests stopping the container is more reliable. A slightly more real-world example would involve choosing a different default organisation name for anonymous access. This example uses \"ChezMoi\". First, the environment key needs to be set to that value: GF_AUTH_ANONYMOUS_ORG_NAME=ChezMoi Then that change needs to be propagated into the Grafana container as explained above. Next, Grafana needs to be told that \"ChezMoi\" is the default organisation: Use your browser to login to Grafana as an administrator. From the \"Server Admin\" slide-out menu on the left hand side, choose \"Orgs\". In the list that appears, click on \"Main Org\". This opens an editing panel. Change the \"Name\" field to \"ChezMoi\" and click \"Update\". Sign-out of Grafana. You will be taken back to the login screen. Your URL bar will look something like this: http://myhost.mydomain.com:3000/login 6. Edit the URL to remove the \"login\" suffix and press return. If all your changes were applied successfully, you will have anonymous access and the URL will look something like this: http://myhost.mydomain.com:3000/?orgId=1","title":"Overriding Grafana settings"},{"location":"Containers/Grafana/#help-i-made-a-mess","text":"\"I made a bit of a mess with Grafana. First time user. Steep learning curve. False starts, many. Mistakes, unavoidable. Been there, done that. But now I really need to start from a clean slate. And, yes, I understand there is no undo for this.\" Begin by stopping Grafana: $ cd ~/IOTstack $ docker-compose stop grafana You have two options: Destroy your settings and dashboards but retain any plugins you may have installed: $ sudo rm ~/IOTstack/volumes/grafana/data/grafana.db Nuke everything (triple-check this command before you hit return): $ sudo rm -rf ~/IOTstack/volumes/grafana/data This is where you should edit ~/IOTstack/services/grafana/grafana.env to correct any problems (such as choosing an administrative username other than \"admin\"). When you are ready, bring Grafana back up again: $ cd ~/IOTstack $ docker-compose up -d Grafana will automatically recreate everything it needs. You will be able to login as \"admin/admin\".","title":"HELP \u2013 I made a mess!"},{"location":"Containers/Home-Assistant/","text":"Home assistant References Docker Webpage Hass.io is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control. Port binding is 8123 . Hass.io is exposed to your hosts' network in order to discover devices on your LAN. That means that it does not sit inside docker's network. To avoid confusion There are 2 versions of Home Assistant: Hass.io and Home Assistant Docker. Hass.io uses its own orchastration with 3 docker images: hassio_supervisor , hassio_dns and homeassistant . Home Assistant Docker runs inside a single docker image, and doesn't support all the features that Hass.io does (such as add-ons). IOTstack currently only has Hass.io and we can only offer limited configuration of it since it is its own platform. More info on versions . Both Hass.io and Home Assistant Docker can be found on the experimental branch of IOTstack. Menu installation Hass.io now has a seperate installation in the menu. The old version was incorrect and should be removed. Be sure to update you project and install the correct version. You will be asked to select you device type during the installation. Hass.io is no longer dependant on the IOTstack, it has its own service for maintaining its uptime. Installation Ensure your system is up to date with: sudo apt update If not already installed, install the network manager with: sudo apt-get install network-manager apparmor-utils before running the hass.io installation to avoid any potential errors. The installation of Hass.io takes up to 20 minutes (depending on your internet connection). Refrain from restarting your machine until it has come online and you are able to create a user account. Removal To remove Hass.io you first need to stop the service that controls it. Run the following in the terminal: sudo systemctl stop hassio-supervisor.service sudo systemctl disable hassio-supervisor.service This should stop the main service however there are two additional container that still need to be address This will stop the service and disable it from starting on the next boot Next you need to stop the hassio_dns and hassio_supervisor docker stop hassio_supervisor docker stop hassio_dns docker stop homeassistant If you want to remove the containers docker rm hassio_supervisor docker rm hassio_dns docker stop homeassistant After rebooting you should be able to reinstall The stored file are located in /usr/share/hassio which can be removed if you need to Double check with docker ps to see if there are other hassio containers running. They can stopped and removed in the same fashion for the dns and supervisor You can use Portainer to view what is running and clean up the unused images.","title":"Home assistant"},{"location":"Containers/Home-Assistant/#home-assistant","text":"","title":"Home assistant"},{"location":"Containers/Home-Assistant/#references","text":"Docker Webpage Hass.io is a home automation platform running on Python 3. It is able to track and control all devices at home and offer a platform for automating control. Port binding is 8123 . Hass.io is exposed to your hosts' network in order to discover devices on your LAN. That means that it does not sit inside docker's network.","title":"References"},{"location":"Containers/Home-Assistant/#to-avoid-confusion","text":"There are 2 versions of Home Assistant: Hass.io and Home Assistant Docker. Hass.io uses its own orchastration with 3 docker images: hassio_supervisor , hassio_dns and homeassistant . Home Assistant Docker runs inside a single docker image, and doesn't support all the features that Hass.io does (such as add-ons). IOTstack currently only has Hass.io and we can only offer limited configuration of it since it is its own platform. More info on versions . Both Hass.io and Home Assistant Docker can be found on the experimental branch of IOTstack.","title":"To avoid confusion"},{"location":"Containers/Home-Assistant/#menu-installation","text":"Hass.io now has a seperate installation in the menu. The old version was incorrect and should be removed. Be sure to update you project and install the correct version. You will be asked to select you device type during the installation. Hass.io is no longer dependant on the IOTstack, it has its own service for maintaining its uptime.","title":"Menu installation"},{"location":"Containers/Home-Assistant/#installation","text":"Ensure your system is up to date with: sudo apt update If not already installed, install the network manager with: sudo apt-get install network-manager apparmor-utils before running the hass.io installation to avoid any potential errors. The installation of Hass.io takes up to 20 minutes (depending on your internet connection). Refrain from restarting your machine until it has come online and you are able to create a user account.","title":"Installation"},{"location":"Containers/Home-Assistant/#removal","text":"To remove Hass.io you first need to stop the service that controls it. Run the following in the terminal: sudo systemctl stop hassio-supervisor.service sudo systemctl disable hassio-supervisor.service This should stop the main service however there are two additional container that still need to be address This will stop the service and disable it from starting on the next boot Next you need to stop the hassio_dns and hassio_supervisor docker stop hassio_supervisor docker stop hassio_dns docker stop homeassistant If you want to remove the containers docker rm hassio_supervisor docker rm hassio_dns docker stop homeassistant After rebooting you should be able to reinstall The stored file are located in /usr/share/hassio which can be removed if you need to Double check with docker ps to see if there are other hassio containers running. They can stopped and removed in the same fashion for the dns and supervisor You can use Portainer to view what is running and clean up the unused images.","title":"Removal"},{"location":"Containers/InfluxDB/","text":"InfluxDB References Docker Website Security The credentials and default database name for influxdb are stored in the file called influxdb/influx.env . The default username and password is set to \"nodered\" for both. It is HIGHLY recommended that you change them. The environment file contains several commented out options allowing you to set several access options such as default admin user credentials as well as the default database name. Any change to the environment file will require a restart of the service. To access the terminal for influxdb execute ./services/influxdb/terminal.sh . Here you can set additional parameters or create other databases.","title":"InfluxDB"},{"location":"Containers/InfluxDB/#influxdb","text":"","title":"InfluxDB"},{"location":"Containers/InfluxDB/#references","text":"Docker Website","title":"References"},{"location":"Containers/InfluxDB/#security","text":"The credentials and default database name for influxdb are stored in the file called influxdb/influx.env . The default username and password is set to \"nodered\" for both. It is HIGHLY recommended that you change them. The environment file contains several commented out options allowing you to set several access options such as default admin user credentials as well as the default database name. Any change to the environment file will require a restart of the service. To access the terminal for influxdb execute ./services/influxdb/terminal.sh . Here you can set additional parameters or create other databases.","title":"Security"},{"location":"Containers/MariaDB/","text":"Source Docker hub Webpage About MariaDB is a fork of MySQL. This is an unofficial image provided by linuxserver.io because there is no official image for arm Conneting to the DB The port is 3306. It exists inside the docker network so you can connect via mariadb:3306 for internal connections. For external connections use <your Pis IP>:3306 Setup Before starting the stack edit the ./services/mariadb/mariadb.env file and set your access details. This is optional however you will only have one shot at the preconfig. If you start the container without setting the passwords then you will have to either delete its volume directory or enter the terminal and change manually The env file has three commented fields for credentials, either all three must be commented or un-commented. You can't have only one or two, its all or nothing. Terminal A terminal is provided to access mariadb by the cli. execute ./services/maraidb/terminal.sh . You will need to run mysql -uroot -p to enter mariadbs interface","title":"MariaDB"},{"location":"Containers/MariaDB/#source","text":"Docker hub Webpage","title":"Source"},{"location":"Containers/MariaDB/#about","text":"MariaDB is a fork of MySQL. This is an unofficial image provided by linuxserver.io because there is no official image for arm","title":"About"},{"location":"Containers/MariaDB/#conneting-to-the-db","text":"The port is 3306. It exists inside the docker network so you can connect via mariadb:3306 for internal connections. For external connections use <your Pis IP>:3306","title":"Conneting to the DB"},{"location":"Containers/MariaDB/#setup","text":"Before starting the stack edit the ./services/mariadb/mariadb.env file and set your access details. This is optional however you will only have one shot at the preconfig. If you start the container without setting the passwords then you will have to either delete its volume directory or enter the terminal and change manually The env file has three commented fields for credentials, either all three must be commented or un-commented. You can't have only one or two, its all or nothing.","title":"Setup"},{"location":"Containers/MariaDB/#terminal","text":"A terminal is provided to access mariadb by the cli. execute ./services/maraidb/terminal.sh . You will need to run mysql -uroot -p to enter mariadbs interface","title":"Terminal"},{"location":"Containers/Mosquitto/","text":"Mosquitto References Docker Website mosquitto.conf documentation Setting up passwords video Definitions docker-compose.yml \u21d2 ~/IOTstack/docker-compose.yml mosquitto.conf \u21d2 ~/IOTstack/services/mosquitto/mosquitto.conf mosquitto.log \u21d2 ~/IOTstack/volumes/mosquitto/log/mosquitto.log service.yml \u21d2 ~/IOTstack/.templates/mosquitto/service.yml volumes/mosquitto \u21d2 ~/IOTstack/volumes/mosquitto/ Logging Mosquitto logging is controlled by mosquitto.conf . This is the default configuration: #log_dest file /mosquitto/log/mosquitto.log # To avoid flash wearing log_dest stdout When log_dest is set to stdout , you inspect Mosquitto's logs like this: $ docker logs mosquitto Logs written to stdout are ephemeral and will disappear when your IOTstack is restarted but this configuration reduces wear and tear on your SD card. The alternative, which may be more appropriate if you are running on an SSD or HD, is to change mosquitto.conf to be like this: log_dest file /mosquitto/log/mosquitto.log # To avoid flash wearing #log_dest stdout and then restart Mosquitto: $ cd ~/IOTstack $ docker-compose restart mosquitto With this configuration, you inspect Mosquitto's logs like this: $ tail ~/IOTstack/volumes/mosquitto/log/mosquitto.log Logs written to mosquitto.log do not disappear when your IOTstack is restarted. They persist until you take action to prune the file. Security By default, the Mosquitto container has no password. You can leave it that way if you like but it's always a good idea to secure your services. Assuming your IOTstack is running: Open a shell in the mosquitto container: $ docker exec -it mosquitto sh In the following, replace \u00abMYUSER\u00bb with the username you want to use for controlling access to Mosquitto and then run these commands: $ mosquitto_passwd -c /mosquitto/pwfile/pwfile \u00abMYUSER\u00bb $ exit mosquitto_passwd will ask you to type a password and confirm it. The path on the right hand side of: -c /mosquitto/pwfile/pwfile is inside the container. Outside the container, it maps to: ~/IOTstack/volumes/mosquitto/pwfile/pwfile You should be able to see the result of setting a username and password like this: $ cat ~/IOTstack/volumes/mosquitto/pwfile/pwfile MYUSER:$6$lBYlxjWtLON0fm96$3qgcEyr/nKvxk3C2Jk36kkILJK7nLdIeLhuywVOVkVbJUjBeqUmCLOA/T6qAq2+hyyJdZ52ALTi+onMEEaM0qQ== $ Open mosquitto.conf in a text editor. Find this line: ``` password_file /mosquitto/pwfile/pwfile ``` Remove the # in front of password_file. Save. Restart Mosquitto: $ cd ~/IOTstack $ docker-compose restart mosquitto Use the new credentials where necessary (eg Node-Red). Notes: You can revert to password-disabled state by going back to step 3, re-inserting the \"#\", then restarting Mosquitto as per step 4. If mosquitto keeps restarting after you implement password checking, the most likely explanation will be something wrong with the password file. Implement the advice in the previous note. Running as root By default, the Mosquitto container is launched as root but then downgrades its privileges to run as user ID 1883. Mosquitto is unusual because most containers just accept the privileges they were launched with. In most cases, that means containers run as root. Don't make the mistake of thinking this means that processes running inside containers can do whatever they like to your host system. A process inside a container is contained . What a process can affect outside its container is governed by the port, device and volume mappings you see in the docker-compose.yml . You can check how mosquitto has been launched like this: $ ps -eo euser,ruser,suser,fuser,comm | grep mosquitto EUSER RUSER SUSER FUSER COMMAND 1883 1883 1883 1883 mosquitto If you have a use-case that needs Mosquitto to run with root privileges: Open docker-compose.yml in a text editor and find this: mosquitto: \u2026 [snip] \u2026 user: \"1883\" change it to: mosquitto: \u2026 [snip] \u2026 user: \"0\" Edit mosquitto.conf to add this line: user root Apply the change: $ cd ~/IOTstack $ docker-compose stop mosquitto $ docker-compose up -d A clean install of Mosquitto via the IOTstack menu sets everything in volumes/mosquitto to user and group 1883. That permission structure will still work if you change Mosquitto to run with root privileges. However, running as root may have the side effect of changing privilege levels within volumes/mosquitto . Keep this in mind if you decide to switch back to running Mosquitto as user 1883 because it is less likely to work. Port 9001 In earlier versions of IOTstack, service.yml included two port mappings which were included in docker-compose.yml when Mosquitto was chosen in the menu: ports: - \"1883:1883\" - \"9001:9001\" Issue 67 explored the topic of port 9001 and showed that: The base image for mosquitto did not expose port 9001; and The running container was not listening to port 9001. On that basis, the mapping for port 9001 was removed from service.yml . If you have a use-case that needs port 9001, you can re-enable support by: Inserting the port mapping under the mosquitto definition in docker-compose.yml : - \"9001:9001\" Inserting the following lines in mosquitto.conf : listener 1883 listener 9001 You need both lines. If you omit 1883 then mosquitto will stop listening to port 1883 and will only listen to port 9001. Restarting the container: $ cd ~/IOTstack $ docker-compose up -d Please consider raising an issue to document your use-case. If you think your use-case has general application then please also consider creating a pull request to make the changes permanent.","title":"Mosquitto"},{"location":"Containers/Mosquitto/#mosquitto","text":"","title":"Mosquitto"},{"location":"Containers/Mosquitto/#references","text":"Docker Website mosquitto.conf documentation Setting up passwords video","title":"References"},{"location":"Containers/Mosquitto/#definitions","text":"docker-compose.yml \u21d2 ~/IOTstack/docker-compose.yml mosquitto.conf \u21d2 ~/IOTstack/services/mosquitto/mosquitto.conf mosquitto.log \u21d2 ~/IOTstack/volumes/mosquitto/log/mosquitto.log service.yml \u21d2 ~/IOTstack/.templates/mosquitto/service.yml volumes/mosquitto \u21d2 ~/IOTstack/volumes/mosquitto/","title":"Definitions"},{"location":"Containers/Mosquitto/#logging","text":"Mosquitto logging is controlled by mosquitto.conf . This is the default configuration: #log_dest file /mosquitto/log/mosquitto.log # To avoid flash wearing log_dest stdout When log_dest is set to stdout , you inspect Mosquitto's logs like this: $ docker logs mosquitto Logs written to stdout are ephemeral and will disappear when your IOTstack is restarted but this configuration reduces wear and tear on your SD card. The alternative, which may be more appropriate if you are running on an SSD or HD, is to change mosquitto.conf to be like this: log_dest file /mosquitto/log/mosquitto.log # To avoid flash wearing #log_dest stdout and then restart Mosquitto: $ cd ~/IOTstack $ docker-compose restart mosquitto With this configuration, you inspect Mosquitto's logs like this: $ tail ~/IOTstack/volumes/mosquitto/log/mosquitto.log Logs written to mosquitto.log do not disappear when your IOTstack is restarted. They persist until you take action to prune the file.","title":"Logging"},{"location":"Containers/Mosquitto/#security","text":"By default, the Mosquitto container has no password. You can leave it that way if you like but it's always a good idea to secure your services. Assuming your IOTstack is running: Open a shell in the mosquitto container: $ docker exec -it mosquitto sh In the following, replace \u00abMYUSER\u00bb with the username you want to use for controlling access to Mosquitto and then run these commands: $ mosquitto_passwd -c /mosquitto/pwfile/pwfile \u00abMYUSER\u00bb $ exit mosquitto_passwd will ask you to type a password and confirm it. The path on the right hand side of: -c /mosquitto/pwfile/pwfile is inside the container. Outside the container, it maps to: ~/IOTstack/volumes/mosquitto/pwfile/pwfile You should be able to see the result of setting a username and password like this: $ cat ~/IOTstack/volumes/mosquitto/pwfile/pwfile MYUSER:$6$lBYlxjWtLON0fm96$3qgcEyr/nKvxk3C2Jk36kkILJK7nLdIeLhuywVOVkVbJUjBeqUmCLOA/T6qAq2+hyyJdZ52ALTi+onMEEaM0qQ== $ Open mosquitto.conf in a text editor. Find this line: ```","title":"Security"},{"location":"Containers/Mosquitto/#password_file-mosquittopwfilepwfile","text":"``` Remove the # in front of password_file. Save. Restart Mosquitto: $ cd ~/IOTstack $ docker-compose restart mosquitto Use the new credentials where necessary (eg Node-Red). Notes: You can revert to password-disabled state by going back to step 3, re-inserting the \"#\", then restarting Mosquitto as per step 4. If mosquitto keeps restarting after you implement password checking, the most likely explanation will be something wrong with the password file. Implement the advice in the previous note.","title":"password_file /mosquitto/pwfile/pwfile"},{"location":"Containers/Mosquitto/#running-as-root","text":"By default, the Mosquitto container is launched as root but then downgrades its privileges to run as user ID 1883. Mosquitto is unusual because most containers just accept the privileges they were launched with. In most cases, that means containers run as root. Don't make the mistake of thinking this means that processes running inside containers can do whatever they like to your host system. A process inside a container is contained . What a process can affect outside its container is governed by the port, device and volume mappings you see in the docker-compose.yml . You can check how mosquitto has been launched like this: $ ps -eo euser,ruser,suser,fuser,comm | grep mosquitto EUSER RUSER SUSER FUSER COMMAND 1883 1883 1883 1883 mosquitto If you have a use-case that needs Mosquitto to run with root privileges: Open docker-compose.yml in a text editor and find this: mosquitto: \u2026 [snip] \u2026 user: \"1883\" change it to: mosquitto: \u2026 [snip] \u2026 user: \"0\" Edit mosquitto.conf to add this line: user root Apply the change: $ cd ~/IOTstack $ docker-compose stop mosquitto $ docker-compose up -d A clean install of Mosquitto via the IOTstack menu sets everything in volumes/mosquitto to user and group 1883. That permission structure will still work if you change Mosquitto to run with root privileges. However, running as root may have the side effect of changing privilege levels within volumes/mosquitto . Keep this in mind if you decide to switch back to running Mosquitto as user 1883 because it is less likely to work.","title":"Running as root"},{"location":"Containers/Mosquitto/#port-9001","text":"In earlier versions of IOTstack, service.yml included two port mappings which were included in docker-compose.yml when Mosquitto was chosen in the menu: ports: - \"1883:1883\" - \"9001:9001\" Issue 67 explored the topic of port 9001 and showed that: The base image for mosquitto did not expose port 9001; and The running container was not listening to port 9001. On that basis, the mapping for port 9001 was removed from service.yml . If you have a use-case that needs port 9001, you can re-enable support by: Inserting the port mapping under the mosquitto definition in docker-compose.yml : - \"9001:9001\" Inserting the following lines in mosquitto.conf : listener 1883 listener 9001 You need both lines. If you omit 1883 then mosquitto will stop listening to port 1883 and will only listen to port 9001. Restarting the container: $ cd ~/IOTstack $ docker-compose up -d Please consider raising an issue to document your use-case. If you think your use-case has general application then please also consider creating a pull request to make the changes permanent.","title":"Port 9001"},{"location":"Containers/MotionEye/","text":"MotionQye References Website About MotionEye is a camera/webcam package. The port is set to 8765 Config This is the yml entry. Notice that the devices is commented out. This is because if you don't have a camera attached then it will fail to start. Uncomment if you need to. This is for a Pi camera, you will need to add additional lines for usb cameras motioneye: image: \"ccrisan/motioneye:master-armhf\" container_name: \"motioneye\" restart: unless-stopped ports: - 8765:8765 - 8081:8081 volumes: - /etc/localtime:/etc/localtime:ro - ./volumes/motioneye/etc_motioneye:/etc/motioneye - ./volumes/motioneye/var_lib_motioneye:/var/lib/motioneye #devices: # - \"/dev/video0:/dev/video0\" Login Details On first login you will be asked for login details. The default user is admin (all lowercase) with no password Storage By default local camera data will be stored in /var/lib/motioneye/camera_name in the container which equates to the following: Remote motioneye If you have connected to a remote motion eye note that the directory is on that device and has nothing to do with the container.","title":"MotionQye"},{"location":"Containers/MotionEye/#motionqye","text":"","title":"MotionQye"},{"location":"Containers/MotionEye/#references","text":"Website","title":"References"},{"location":"Containers/MotionEye/#about","text":"MotionEye is a camera/webcam package. The port is set to 8765","title":"About"},{"location":"Containers/MotionEye/#config","text":"This is the yml entry. Notice that the devices is commented out. This is because if you don't have a camera attached then it will fail to start. Uncomment if you need to. This is for a Pi camera, you will need to add additional lines for usb cameras motioneye: image: \"ccrisan/motioneye:master-armhf\" container_name: \"motioneye\" restart: unless-stopped ports: - 8765:8765 - 8081:8081 volumes: - /etc/localtime:/etc/localtime:ro - ./volumes/motioneye/etc_motioneye:/etc/motioneye - ./volumes/motioneye/var_lib_motioneye:/var/lib/motioneye #devices: # - \"/dev/video0:/dev/video0\"","title":"Config"},{"location":"Containers/MotionEye/#login-details","text":"On first login you will be asked for login details. The default user is admin (all lowercase) with no password","title":"Login Details"},{"location":"Containers/MotionEye/#storage","text":"By default local camera data will be stored in /var/lib/motioneye/camera_name in the container which equates to the following:","title":"Storage"},{"location":"Containers/MotionEye/#remote-motioneye","text":"If you have connected to a remote motion eye note that the directory is on that device and has nothing to do with the container.","title":"Remote motioneye"},{"location":"Containers/NextCloud/","text":"Next Cloud DO NOT EXPOSE PORT 80 TO THE WEB It is a very bad idea to expose unencrypted traffic to the web. You will need to use a reverse-proxy to ensure your password is not stolen and your account hacked. I'm still working on getting a good encrypted reverse proxy working. However in the interim you can use a VPN tunnel like OpenVPN or Zerotier to securely connect to your private cloud Backups Nextcloud has been excluded from the docker_backup script due to its potential size. Once I've found a better way of backing it up I will add a dedicated script for it. Setup Next-Cloud recommends using MySQL/MariaDB for the accounts and file list. The alternative is to use SQLite however they strongly discourage using it This is the service yml. Notice that there are in fact two containers, one for the db and the other for the cloud itself. You will need to change the passwords before starting the stack (remember to change the docker-compose.yml and ./services/nextcloud/service.yml), if you dont you will need to delete the volume directory and start again. nextcloud: image: nextcloud container_name: nextcloud ports: - 9321:80 volumes: - ./volumes/nextcloud/html:/var/www/html restart: unless-stopped depends_on: - nextcloud_db nextcloud_db: image: linuxserver/mariadb container_name: nextcloud_db volumes: - ./volumes/nextcloud/db:/config environment: - MYSQL_ROOT_PASSWORD=stronger_password - MYSQL_PASSWORD=strong_password - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud The port is 9321 click on the storage options, select maraiadb/mysql and fill in the details as follows Note that you data will be stored in ./volumes/nextcloud/html/data/{account} Also note that file permissions are \"www-data\" so you cant simply copy data into this folder directly, you should use the web interface or the app. It would be a good idea to mount an external drive to store the data in rather than on your sd card. details to follow shortly. Something like: The external drive will have to be an ext4 formatted drive because smb, fat32 and NTFS can't handle linux file permissions. If the permissions aren't set to \"www-data\" then the container wont be able to write to the disk. Just a warning: If your database gets corrupted then your nextcloud is pretty much stuffed","title":"Next Cloud"},{"location":"Containers/NextCloud/#next-cloud","text":"","title":"Next Cloud"},{"location":"Containers/NextCloud/#do-not-expose-port-80-to-the-web","text":"It is a very bad idea to expose unencrypted traffic to the web. You will need to use a reverse-proxy to ensure your password is not stolen and your account hacked. I'm still working on getting a good encrypted reverse proxy working. However in the interim you can use a VPN tunnel like OpenVPN or Zerotier to securely connect to your private cloud","title":"DO NOT EXPOSE PORT 80 TO THE WEB"},{"location":"Containers/NextCloud/#backups","text":"Nextcloud has been excluded from the docker_backup script due to its potential size. Once I've found a better way of backing it up I will add a dedicated script for it.","title":"Backups"},{"location":"Containers/NextCloud/#setup","text":"Next-Cloud recommends using MySQL/MariaDB for the accounts and file list. The alternative is to use SQLite however they strongly discourage using it This is the service yml. Notice that there are in fact two containers, one for the db and the other for the cloud itself. You will need to change the passwords before starting the stack (remember to change the docker-compose.yml and ./services/nextcloud/service.yml), if you dont you will need to delete the volume directory and start again. nextcloud: image: nextcloud container_name: nextcloud ports: - 9321:80 volumes: - ./volumes/nextcloud/html:/var/www/html restart: unless-stopped depends_on: - nextcloud_db nextcloud_db: image: linuxserver/mariadb container_name: nextcloud_db volumes: - ./volumes/nextcloud/db:/config environment: - MYSQL_ROOT_PASSWORD=stronger_password - MYSQL_PASSWORD=strong_password - MYSQL_DATABASE=nextcloud - MYSQL_USER=nextcloud The port is 9321 click on the storage options, select maraiadb/mysql and fill in the details as follows Note that you data will be stored in ./volumes/nextcloud/html/data/{account} Also note that file permissions are \"www-data\" so you cant simply copy data into this folder directly, you should use the web interface or the app. It would be a good idea to mount an external drive to store the data in rather than on your sd card. details to follow shortly. Something like: The external drive will have to be an ext4 formatted drive because smb, fat32 and NTFS can't handle linux file permissions. If the permissions aren't set to \"www-data\" then the container wont be able to write to the disk. Just a warning: If your database gets corrupted then your nextcloud is pretty much stuffed","title":"Setup"},{"location":"Containers/Node-RED/","text":"Node-RED References Docker website Build warning The Node-RED build will complain about several issues. This is completely normal behaviour. SQLite Thanks to @fragolinux the SQLite node will install now. WARNING it will output many error and will look as if it has gotten stuck. Just give it time and it will continue. GPIO To communicate to your Pi's GPIO you need to use the node-red-node-pi-gpiod node. It allowes you to connect to multiple Pis from the same nodered service. You need to make sure that pigpdiod is running. The recommended method is listed here You run the following command sudo nano /etc/rc.local and add the line /usr/bin/pigpiod above exit 0 and reboot the Pi. There is an option to secure the service see the writeup for further instuctions. Fot the Rpi Image you will also need to update to the most recent version sudo apt-get update sudo apt-get install pigpio python-pigpio python3-pigpio Drop the gpio node and use your Pi's IP. Example: 192.168.1.123 (127.0.0.1 won't work because this is the local address of every computer'.) Securing Node-RED To secure Node-RED you need a password hash. There is a terminal script ./services/nodered/terminal.sh execute it to get into the terminal. Copy the helper text node -e ..... PASSWORD , paste it and change your password to get a hash. Open the file ./volumes/nodered/data/settings.js and follow the writeup on https://nodered.org/docs/user-guide/runtime/securing-node-red for further instructions Sharing files between Node-RED and the host Containers run in a sandboxed environment they can't see the host (the Pi itself) file system. This presents a problem if you want to read a file directly to the host from inside the container. Fortunately there is a method, the containers have been set up with volume mapping. The volume maps a specific directory or file from the host file system into the container. Therefore if you write to that directory both the host and the container can see the files. Consider the following: The docker-compose.yml file shows the following for Node-RED volumes: - ./volumes/nodered/data:/data If inside Node-RED you were to write to the /data folder then the host would see it in ~/IOTstack/volumes/nodered/data (the ./volumes above implies relative to the docker-compose.yml file) The flow writes the file /data/test.txt and it is visible in the host as ~/IOTstack/volumes/nodered/data/test.txt Remember, files and directories in the volume are persistent between restarts. If you save your data elsewhere it will be destroyed should you restart. Creating a subdirectory in volume i.e. /data/storage/ would be advised Using Bluetooth In order to allow Node-RED to access the Pi's Bluetooth module the docker-comose.yml file needs to be modified to allow it access. network_mode: \"host\" needs to be added (make sure the indentation is correct, us spaces not tabs): nodered: container_name: nodered build: ./services/nodered/. restart: unless-stopped user: \"0\" network_mode: \"host\" By activating host mode the Node-RED container can no longer access containers by name http://influxdb:8086 will no longer work. Node-RED thinks it now is the host and therefore access to the following services will look as follows: * influxdb http://127.0.0.1:8086 * GPIO 127.0.0.1 port 8888 * MQTT 127.0.0.1 Unused node in Protainer Portainer will report that the nodered image is unsed, this is normal due to the method used build the image. This is normal behavior. It is not advised to remove it as it is used as the base for the iotstack_nodered image, you will need to redownload it should you rebuild the nodered image. Running the exec node against the host Pi Due to the isolation between containers and the host the exec node will run against the container. There is a solution to work around this. You can use ssh to execute a script on the pi. It requires a little setup but is possible. For this example I'll be running a simple script called test.sh I create a file called test.sh in my IOTstack directory with nano The contents are as follows: #!/bin/bash echo \"hello\" exit 0 The exit 0 will stop the exec node from reporting an issue. Its a good idea to add the shebang at the top. make it executable with chmod +x test.sh This nodered running open the nodered terminal with ./services/nodered/terminal.sh or docker exec -it nodered /bin/bash or use portainer create the ssh folder in the data directory (the /data directory is persistently mapped volume) mkdir -p /data/ssh create key, this will require naming the output file ssh-keygen -f /data/ssh/nodered put in any additional config you want key type strength copy the key to the Pi. When asked for a password leave it blank copy the ssh-key to your pi ssh-copy-id -i /data/ssh/nodered pi@192.168.x.x replace with your static IP address. You will have to reply yes to the prompt. You may also see an error referring to regular expressions however you can ignore it. now to execute a script on the pi run ssh -i /data/ssh/nodered pi@192.168.x.x /home/pi/IOTstack/test.sh type exit to leave the terminal (you could also restart your pi with ssh -i /data/ssh/nodered pi@192.168.x.x sudo reboot ) in node red in your exec node you can run the command ssh -i /data/ssh/nodered pi@192.168.x.x /home/pi/IOTstack/test.sh other the script or command of your choice","title":"Node-RED"},{"location":"Containers/Node-RED/#node-red","text":"","title":"Node-RED"},{"location":"Containers/Node-RED/#references","text":"Docker website","title":"References"},{"location":"Containers/Node-RED/#build-warning","text":"The Node-RED build will complain about several issues. This is completely normal behaviour.","title":"Build warning"},{"location":"Containers/Node-RED/#sqlite","text":"Thanks to @fragolinux the SQLite node will install now. WARNING it will output many error and will look as if it has gotten stuck. Just give it time and it will continue.","title":"SQLite"},{"location":"Containers/Node-RED/#gpio","text":"To communicate to your Pi's GPIO you need to use the node-red-node-pi-gpiod node. It allowes you to connect to multiple Pis from the same nodered service. You need to make sure that pigpdiod is running. The recommended method is listed here You run the following command sudo nano /etc/rc.local and add the line /usr/bin/pigpiod above exit 0 and reboot the Pi. There is an option to secure the service see the writeup for further instuctions. Fot the Rpi Image you will also need to update to the most recent version sudo apt-get update sudo apt-get install pigpio python-pigpio python3-pigpio Drop the gpio node and use your Pi's IP. Example: 192.168.1.123 (127.0.0.1 won't work because this is the local address of every computer'.)","title":"GPIO"},{"location":"Containers/Node-RED/#securing-node-red","text":"To secure Node-RED you need a password hash. There is a terminal script ./services/nodered/terminal.sh execute it to get into the terminal. Copy the helper text node -e ..... PASSWORD , paste it and change your password to get a hash. Open the file ./volumes/nodered/data/settings.js and follow the writeup on https://nodered.org/docs/user-guide/runtime/securing-node-red for further instructions","title":"Securing Node-RED"},{"location":"Containers/Node-RED/#sharing-files-between-node-red-and-the-host","text":"Containers run in a sandboxed environment they can't see the host (the Pi itself) file system. This presents a problem if you want to read a file directly to the host from inside the container. Fortunately there is a method, the containers have been set up with volume mapping. The volume maps a specific directory or file from the host file system into the container. Therefore if you write to that directory both the host and the container can see the files. Consider the following: The docker-compose.yml file shows the following for Node-RED volumes: - ./volumes/nodered/data:/data If inside Node-RED you were to write to the /data folder then the host would see it in ~/IOTstack/volumes/nodered/data (the ./volumes above implies relative to the docker-compose.yml file) The flow writes the file /data/test.txt and it is visible in the host as ~/IOTstack/volumes/nodered/data/test.txt Remember, files and directories in the volume are persistent between restarts. If you save your data elsewhere it will be destroyed should you restart. Creating a subdirectory in volume i.e. /data/storage/ would be advised","title":"Sharing files between Node-RED and the host"},{"location":"Containers/Node-RED/#using-bluetooth","text":"In order to allow Node-RED to access the Pi's Bluetooth module the docker-comose.yml file needs to be modified to allow it access. network_mode: \"host\" needs to be added (make sure the indentation is correct, us spaces not tabs): nodered: container_name: nodered build: ./services/nodered/. restart: unless-stopped user: \"0\" network_mode: \"host\" By activating host mode the Node-RED container can no longer access containers by name http://influxdb:8086 will no longer work. Node-RED thinks it now is the host and therefore access to the following services will look as follows: * influxdb http://127.0.0.1:8086 * GPIO 127.0.0.1 port 8888 * MQTT 127.0.0.1","title":"Using Bluetooth"},{"location":"Containers/Node-RED/#unused-node-in-protainer","text":"Portainer will report that the nodered image is unsed, this is normal due to the method used build the image. This is normal behavior. It is not advised to remove it as it is used as the base for the iotstack_nodered image, you will need to redownload it should you rebuild the nodered image.","title":"Unused node in Protainer"},{"location":"Containers/Node-RED/#running-the-exec-node-against-the-host-pi","text":"Due to the isolation between containers and the host the exec node will run against the container. There is a solution to work around this. You can use ssh to execute a script on the pi. It requires a little setup but is possible. For this example I'll be running a simple script called test.sh I create a file called test.sh in my IOTstack directory with nano The contents are as follows: #!/bin/bash echo \"hello\" exit 0 The exit 0 will stop the exec node from reporting an issue. Its a good idea to add the shebang at the top. make it executable with chmod +x test.sh This nodered running open the nodered terminal with ./services/nodered/terminal.sh or docker exec -it nodered /bin/bash or use portainer create the ssh folder in the data directory (the /data directory is persistently mapped volume) mkdir -p /data/ssh create key, this will require naming the output file ssh-keygen -f /data/ssh/nodered put in any additional config you want key type strength copy the key to the Pi. When asked for a password leave it blank copy the ssh-key to your pi ssh-copy-id -i /data/ssh/nodered pi@192.168.x.x replace with your static IP address. You will have to reply yes to the prompt. You may also see an error referring to regular expressions however you can ignore it. now to execute a script on the pi run ssh -i /data/ssh/nodered pi@192.168.x.x /home/pi/IOTstack/test.sh type exit to leave the terminal (you could also restart your pi with ssh -i /data/ssh/nodered pi@192.168.x.x sudo reboot ) in node red in your exec node you can run the command ssh -i /data/ssh/nodered pi@192.168.x.x /home/pi/IOTstack/test.sh other the script or command of your choice","title":"Running the exec node against the host Pi"},{"location":"Containers/Pi-hole/","text":"Pi-hole Pi-hole is a fantastic utility to reduce ads The interface can be found on \"your_ip\":8089/admin Default password is pihole . This can be changed in the ~/IOTstack/services/pihole/pihole.env file To enable your router to use the pihole container edit your DNS settings on your router to point to your Pi's IP address","title":"Pi-hole"},{"location":"Containers/Pi-hole/#pi-hole","text":"Pi-hole is a fantastic utility to reduce ads The interface can be found on \"your_ip\":8089/admin Default password is pihole . This can be changed in the ~/IOTstack/services/pihole/pihole.env file To enable your router to use the pihole container edit your DNS settings on your router to point to your Pi's IP address","title":"Pi-hole"},{"location":"Containers/Plex/","text":"Plex References Homepage Docker Web interface The web UI can be found on \"your_ip\":32400/web Mounting an external drive by UUID to the home directory official mounting guide Create a directory in you home directory called mnt with a subdirectory HDD . Follow the instruction above to mount your external drive to /home/pi/mnt/HDD in you fstab edit your docker-compose.yml file under plex and uncomment the volumes for tv series and movies (modify the path to point to your media locations). Run docker-compose up -d to rebuild plex with the new volumes","title":"Plex"},{"location":"Containers/Plex/#plex","text":"","title":"Plex"},{"location":"Containers/Plex/#references","text":"Homepage Docker","title":"References"},{"location":"Containers/Plex/#web-interface","text":"The web UI can be found on \"your_ip\":32400/web","title":"Web interface"},{"location":"Containers/Plex/#mounting-an-external-drive-by-uuid-to-the-home-directory","text":"official mounting guide Create a directory in you home directory called mnt with a subdirectory HDD . Follow the instruction above to mount your external drive to /home/pi/mnt/HDD in you fstab edit your docker-compose.yml file under plex and uncomment the volumes for tv series and movies (modify the path to point to your media locations). Run docker-compose up -d to rebuild plex with the new volumes","title":"Mounting an external drive by UUID to the home directory"},{"location":"Containers/Portainer-agent/","text":"Portainer agent References Docker Docs About The protainer agent is a great way to add a second docker instance to a existing portainer instance. this allows you to mananage multiple docker enviroments form one prortainer instance Adding to an existing instance When you want to add the the agent to an existing portianer instance. You go to the endpoints tab. Click on Add endpoint Select Agent Enter the name of the agent Enter the url of the endpoint ip-of-agent-instance:9001 Click on add endpoint","title":"Portainer agent"},{"location":"Containers/Portainer-agent/#portainer-agent","text":"","title":"Portainer agent"},{"location":"Containers/Portainer-agent/#references","text":"Docker Docs","title":"References"},{"location":"Containers/Portainer-agent/#about","text":"The protainer agent is a great way to add a second docker instance to a existing portainer instance. this allows you to mananage multiple docker enviroments form one prortainer instance","title":"About"},{"location":"Containers/Portainer-agent/#adding-to-an-existing-instance","text":"When you want to add the the agent to an existing portianer instance. You go to the endpoints tab. Click on Add endpoint Select Agent Enter the name of the agent Enter the url of the endpoint ip-of-agent-instance:9001 Click on add endpoint","title":"Adding to an existing instance"},{"location":"Containers/Portainer/","text":"Portainer References Docker Website Portainer restart by itself There is an issue with the armhf Portainer image where it randomly restarts. This does not affect its operation. The bug has been reported. About Portainer is a great application for managing Docker. In your web browser navigate to #yourip:9000 . You will be asked to choose a password. In the next window select 'Local' and connect, it shouldn't ask you this again. From here you can play around, click local, and take a look around. This can help you find unused images/containers. On the Containers section, there are 'Quick actions' to view logs and other stats. Note: This can all be done from the CLI but portainer just makes it much much easier. Setup Public IP When you first run Portainer and navigate to the Containers list you will see that there is a clickable link to the ports however this will direct you to 0.0.0.0:port . This is because Portainer doesn't know your IP address. This can be set in the endpoint and set the public IP Forgotten password If you have forgotten the password you created for the container, stop the stack remove portainers volume with sudo rm -r ./volumes/portainer and start the stack. Your browser may get a little confused when it restarts. Just navigate to \"yourip:9000\" (may require more than one attempt) and create your new login details. If it doesn't ask you to connect to the 'Local' docker or shows an empty endpoint just logout and log back in and it will give you the option. From now on it should just work fine.","title":"Portainer"},{"location":"Containers/Portainer/#portainer","text":"","title":"Portainer"},{"location":"Containers/Portainer/#references","text":"Docker Website","title":"References"},{"location":"Containers/Portainer/#portainer-restart-by-itself","text":"There is an issue with the armhf Portainer image where it randomly restarts. This does not affect its operation. The bug has been reported.","title":"Portainer restart by itself"},{"location":"Containers/Portainer/#about","text":"Portainer is a great application for managing Docker. In your web browser navigate to #yourip:9000 . You will be asked to choose a password. In the next window select 'Local' and connect, it shouldn't ask you this again. From here you can play around, click local, and take a look around. This can help you find unused images/containers. On the Containers section, there are 'Quick actions' to view logs and other stats. Note: This can all be done from the CLI but portainer just makes it much much easier.","title":"About"},{"location":"Containers/Portainer/#setup-public-ip","text":"When you first run Portainer and navigate to the Containers list you will see that there is a clickable link to the ports however this will direct you to 0.0.0.0:port . This is because Portainer doesn't know your IP address. This can be set in the endpoint and set the public IP","title":"Setup Public IP"},{"location":"Containers/Portainer/#forgotten-password","text":"If you have forgotten the password you created for the container, stop the stack remove portainers volume with sudo rm -r ./volumes/portainer and start the stack. Your browser may get a little confused when it restarts. Just navigate to \"yourip:9000\" (may require more than one attempt) and create your new login details. If it doesn't ask you to connect to the 'Local' docker or shows an empty endpoint just logout and log back in and it will give you the option. From now on it should just work fine.","title":"Forgotten password"},{"location":"Containers/PostgreSQL/","text":"PostgreSQL References Docker Website About PostgreSQL is an SQL server, for those that need an SQL database. The database credentials can be found in the file ./volumes/postgres/postgres.env . It is highly recommended to change the user, password and default database","title":"PostgreSQL"},{"location":"Containers/PostgreSQL/#postgresql","text":"","title":"PostgreSQL"},{"location":"Containers/PostgreSQL/#references","text":"Docker Website","title":"References"},{"location":"Containers/PostgreSQL/#about","text":"PostgreSQL is an SQL server, for those that need an SQL database. The database credentials can be found in the file ./volumes/postgres/postgres.env . It is highly recommended to change the user, password and default database","title":"About"},{"location":"Containers/Python/","text":"Python Docker hub Running python code in docker In order to run code in docker the container needs to be build from a Dockerfile. There are 2 key files in the service directory services/python/requirements.txt Normally on your system you would install modules with pip and they would be available system wide. The container that comes off Docker hub is blank and we will have to install them and bake them into the container. Before your first run add the modules that you require to the requirements.txt, each on a new line flask bs4 IMPORTANT : Every time you alter the requirements file you will need to rebuild the container and bake in the new modules To build the container run docker-compose build python . services/python/service.yml This is the template that gets concatenated into docker-compose.yml and there are a few things to note here python: container_name: python build: ./services/python/. restart: unless-stopped network_mode: host volumes: - ./volumes/python/app:/usr/src/app The container runs in host network mode. This is because i have no idea which ports you want to use. The implication of this is you will not be able to connect by name to the other container and therefore if you want to connect to the mqtt service or influx you will need to use localhost or 127.0.0.1 because the python container \"thinks\" from network perspective that it is the Pi The container is set to restart unless stopped. Therefore if you write an application it will effectively execute in an endless loop. If you only want a run once method then you will need to comment out the \"restart\" section in the docker-compose.yml file and the service.yml Where to put your code You will need to copy your code to IOTstack/volumes/python/app . The container is set to execute app.py as the main file. writing to the console If you execute a print statement the text will appear in the console of the container. The output can be accessed by running docker logs python writing to disk Inside the container the working directory is /usr/src/app as mapped in the volume command. It would be advised to read or write any data from this directory. Image clutter Doing multiple builds of the python image will create many unused images. These can be cleaned up inside portainer or by running ./scripts/prune-images.sh","title":"Python"},{"location":"Containers/Python/#python","text":"Docker hub","title":"Python"},{"location":"Containers/Python/#running-python-code-in-docker","text":"In order to run code in docker the container needs to be build from a Dockerfile. There are 2 key files in the service directory","title":"Running python code in docker"},{"location":"Containers/Python/#servicespythonrequirementstxt","text":"Normally on your system you would install modules with pip and they would be available system wide. The container that comes off Docker hub is blank and we will have to install them and bake them into the container. Before your first run add the modules that you require to the requirements.txt, each on a new line flask bs4 IMPORTANT : Every time you alter the requirements file you will need to rebuild the container and bake in the new modules To build the container run docker-compose build python .","title":"services/python/requirements.txt"},{"location":"Containers/Python/#servicespythonserviceyml","text":"This is the template that gets concatenated into docker-compose.yml and there are a few things to note here python: container_name: python build: ./services/python/. restart: unless-stopped network_mode: host volumes: - ./volumes/python/app:/usr/src/app The container runs in host network mode. This is because i have no idea which ports you want to use. The implication of this is you will not be able to connect by name to the other container and therefore if you want to connect to the mqtt service or influx you will need to use localhost or 127.0.0.1 because the python container \"thinks\" from network perspective that it is the Pi The container is set to restart unless stopped. Therefore if you write an application it will effectively execute in an endless loop. If you only want a run once method then you will need to comment out the \"restart\" section in the docker-compose.yml file and the service.yml","title":"services/python/service.yml"},{"location":"Containers/Python/#where-to-put-your-code","text":"You will need to copy your code to IOTstack/volumes/python/app . The container is set to execute app.py as the main file.","title":"Where to put your code"},{"location":"Containers/Python/#writing-to-the-console","text":"If you execute a print statement the text will appear in the console of the container. The output can be accessed by running docker logs python","title":"writing to the console"},{"location":"Containers/Python/#writing-to-disk","text":"Inside the container the working directory is /usr/src/app as mapped in the volume command. It would be advised to read or write any data from this directory.","title":"writing to disk"},{"location":"Containers/Python/#image-clutter","text":"Doing multiple builds of the python image will create many unused images. These can be cleaned up inside portainer or by running ./scripts/prune-images.sh","title":"Image clutter"},{"location":"Containers/RTL_433-docker/","text":"RTL_433 Docker Requirements, you will need to have a SDR dongle for you to be able to use RTL. I've tested this with a RTL2838 Make sure you can see your receiver by running lsusb $ lsusb Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 004: ID 0bda:2838 Realtek Semiconductor Corp. RTL2838 DVB-T Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Before starting the container please install RTL_433 from the native installs menu. This will setup your environment with the correct variables and programs. It is also advised to run RTL_433 to verify that it is working correctly on your system. The container is designed to send all detected messages over mqtt Edit the IOTstack/services/rtl_433/rtl_433.env file with your relevant settings for your mqtt server: MQTT_ADDRESS=mosquitto MQTT_PORT=1833 #MQTT_USER=myuser #MQTT_PASSWORD=mypassword MQTT_TOPIC=RTL_433 the container starts with the command rtl_433 -F mqtt:.... currently it does not filter any packets, you will need to do this in Node-RED","title":"RTL_433 Docker"},{"location":"Containers/RTL_433-docker/#rtl_433-docker","text":"Requirements, you will need to have a SDR dongle for you to be able to use RTL. I've tested this with a RTL2838 Make sure you can see your receiver by running lsusb $ lsusb Bus 003 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Bus 002 Device 001: ID 1d6b:0003 Linux Foundation 3.0 root hub Bus 001 Device 004: ID 0bda:2838 Realtek Semiconductor Corp. RTL2838 DVB-T Bus 001 Device 002: ID 2109:3431 VIA Labs, Inc. Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub Before starting the container please install RTL_433 from the native installs menu. This will setup your environment with the correct variables and programs. It is also advised to run RTL_433 to verify that it is working correctly on your system. The container is designed to send all detected messages over mqtt Edit the IOTstack/services/rtl_433/rtl_433.env file with your relevant settings for your mqtt server: MQTT_ADDRESS=mosquitto MQTT_PORT=1833 #MQTT_USER=myuser #MQTT_PASSWORD=mypassword MQTT_TOPIC=RTL_433 the container starts with the command rtl_433 -F mqtt:.... currently it does not filter any packets, you will need to do this in Node-RED","title":"RTL_433 Docker"},{"location":"Containers/TasmoAdmin/","text":"TasmoAdmin References Homepage Docker Web interface The web UI can be found on \"your_ip\":8088 Usage (instructions to follow)","title":"TasmoAdmin"},{"location":"Containers/TasmoAdmin/#tasmoadmin","text":"","title":"TasmoAdmin"},{"location":"Containers/TasmoAdmin/#references","text":"Homepage Docker","title":"References"},{"location":"Containers/TasmoAdmin/#web-interface","text":"The web UI can be found on \"your_ip\":8088","title":"Web interface"},{"location":"Containers/TasmoAdmin/#usage","text":"(instructions to follow)","title":"Usage"},{"location":"Containers/Zigbee2MQTT/","text":"Zigbe2MQTT Web Guide First startup After starting the stack check to see if there is an error due to missing device. This is because the devices are mapped differently on the Pi. If your device is not showing in the container then you can also follow the followings steps. If you get a startup failure open the docker-compose.yml file and under zigbee2mqtt change this: devices: - /dev/ttyAMA0:/dev/ttyACM0 #- /dev/ttyACM0:/dev/ttyACM0 to devices: #- /dev/ttyAMA0:/dev/ttyACM0 - /dev/ttyACM0:/dev/ttyACM0 and run docker-compose up -d again If the container starts then run docker logs zigbe2mqtt so see the log output and if your device is recognised. You may need to reset the device for docker to see it. To edit the configuration file sudo nano volumes/zigbee2mqtt/data/configuration.yaml you many need to restart the container for changes to take affect docker-compose restart zigbee2mqtt Unfortunately I don't own a zigbee device and cannot offer support on the setup you will need to follow the website instructions for further instructions https://www.zigbee2mqtt.io/ terminal access to access the terminal run docker exec -it zigbee2mqtt /bin/sh or select `/bin/sh","title":"Zigbe2MQTT"},{"location":"Containers/Zigbee2MQTT/#zigbe2mqtt","text":"Web Guide","title":"Zigbe2MQTT"},{"location":"Containers/Zigbee2MQTT/#first-startup","text":"After starting the stack check to see if there is an error due to missing device. This is because the devices are mapped differently on the Pi. If your device is not showing in the container then you can also follow the followings steps. If you get a startup failure open the docker-compose.yml file and under zigbee2mqtt change this: devices: - /dev/ttyAMA0:/dev/ttyACM0 #- /dev/ttyACM0:/dev/ttyACM0 to devices: #- /dev/ttyAMA0:/dev/ttyACM0 - /dev/ttyACM0:/dev/ttyACM0 and run docker-compose up -d again If the container starts then run docker logs zigbe2mqtt so see the log output and if your device is recognised. You may need to reset the device for docker to see it. To edit the configuration file sudo nano volumes/zigbee2mqtt/data/configuration.yaml you many need to restart the container for changes to take affect docker-compose restart zigbee2mqtt Unfortunately I don't own a zigbee device and cannot offer support on the setup you will need to follow the website instructions for further instructions https://www.zigbee2mqtt.io/","title":"First startup"},{"location":"Containers/Zigbee2MQTT/#terminal-access","text":"to access the terminal run docker exec -it zigbee2mqtt /bin/sh or select `/bin/sh","title":"terminal access"},{"location":"Containers/deconz/","text":"deCONZ References Docker Website Troubleshooting Make sure your Conbee/Conbee II/RaspBee gateway is connected. If your gateway is not detected, or no lights can be paired, try moving the device to another usb port, reboot your computer and build the stack from the menu again cd ~/IOTstack && bash ./menu.sh (select \"Pull full service from template\" if prompted). The gateway must be plugged in when the deCONZ Docker container is being built. Before running docker-compose up -d , make sure your Linux user is part of the dialout group, which allows the user access to serial devices (i.e. Conbee/Conbee II/RaspBee). If you are not certain, simply add your user to the dialout group by running the following command (username \"pi\" being used as an example): sudo usermod -a -G dialout pi Now run docker-compose up -d to build the stack. If you are still experiencing issues, run docker-compose down to remove all containers from the stack and then docker-compose up -d to build them again. Use a 0.5-1m usb extension cable with ConBee (II) to avoid wifi and bluetooth noise/interference from your Raspberry Pi (recommended by the manufacturer and often the solution to poor performance). Accessing the Phoscon UI The Phoscon UI is available using port 8090 (http://your.local.ip.address:8090/) Viewing the deCONZ Zigbee mesh The Zigbee mesh can be viewed using VNC on port 5901. The default VNC password is \"changeme\". Connecting deCONZ and Node-RED Install node-red-contrib-deconz via the \"Manage palette\" menu in Node-RED (if not already installed) and follow these 2 simple steps (also shown in the video below): Step 1: In the Phoscon UI, Go to Settings > Gateway > Advanced and click \"Authenticate app\". Step 2: In Node-RED, open a deCONZ node, select \"Add new deonz-server\", insert your ip adress and port 8090 and click \"Get settings\". Click \"Add\", \"Done\" and \"Deploy\". Your device list will not be updated before deploying.","title":"deCONZ"},{"location":"Containers/deconz/#deconz","text":"","title":"deCONZ"},{"location":"Containers/deconz/#references","text":"Docker Website","title":"References"},{"location":"Containers/deconz/#troubleshooting","text":"Make sure your Conbee/Conbee II/RaspBee gateway is connected. If your gateway is not detected, or no lights can be paired, try moving the device to another usb port, reboot your computer and build the stack from the menu again cd ~/IOTstack && bash ./menu.sh (select \"Pull full service from template\" if prompted). The gateway must be plugged in when the deCONZ Docker container is being built. Before running docker-compose up -d , make sure your Linux user is part of the dialout group, which allows the user access to serial devices (i.e. Conbee/Conbee II/RaspBee). If you are not certain, simply add your user to the dialout group by running the following command (username \"pi\" being used as an example): sudo usermod -a -G dialout pi Now run docker-compose up -d to build the stack. If you are still experiencing issues, run docker-compose down to remove all containers from the stack and then docker-compose up -d to build them again. Use a 0.5-1m usb extension cable with ConBee (II) to avoid wifi and bluetooth noise/interference from your Raspberry Pi (recommended by the manufacturer and often the solution to poor performance).","title":"Troubleshooting"},{"location":"Containers/deconz/#accessing-the-phoscon-ui","text":"The Phoscon UI is available using port 8090 (http://your.local.ip.address:8090/)","title":"Accessing the Phoscon UI"},{"location":"Containers/deconz/#viewing-the-deconz-zigbee-mesh","text":"The Zigbee mesh can be viewed using VNC on port 5901. The default VNC password is \"changeme\".","title":"Viewing the deCONZ Zigbee mesh"},{"location":"Containers/deconz/#connecting-deconz-and-node-red","text":"Install node-red-contrib-deconz via the \"Manage palette\" menu in Node-RED (if not already installed) and follow these 2 simple steps (also shown in the video below): Step 1: In the Phoscon UI, Go to Settings > Gateway > Advanced and click \"Authenticate app\". Step 2: In Node-RED, open a deCONZ node, select \"Add new deonz-server\", insert your ip adress and port 8090 and click \"Get settings\". Click \"Add\", \"Done\" and \"Deploy\". Your device list will not be updated before deploying.","title":"Connecting deCONZ and Node-RED"},{"location":"Containers/diyHue/","text":"DIY hue website About diyHue is a utility to contol the lights in your home Setup Before you start diyHue you will need to get your IP and MAC addresses. Run ip addr in the terminal Enter these values into the ./services/diyhue/diyhue.env file The default username and password it Hue and Hue respectively Usage The web interface is available on port 8070","title":"DIY hue"},{"location":"Containers/diyHue/#diy-hue","text":"website","title":"DIY hue"},{"location":"Containers/diyHue/#about","text":"diyHue is a utility to contol the lights in your home","title":"About"},{"location":"Containers/diyHue/#setup","text":"Before you start diyHue you will need to get your IP and MAC addresses. Run ip addr in the terminal Enter these values into the ./services/diyhue/diyhue.env file The default username and password it Hue and Hue respectively","title":"Setup"},{"location":"Containers/diyHue/#usage","text":"The web interface is available on port 8070","title":"Usage"},{"location":"Containers/openHAB/","text":"Openhab References Docker website openHAB has been added without Amazon Dashbutton binding. Port binding is 8080 for http and 8443 for https.","title":"Openhab"},{"location":"Containers/openHAB/#openhab","text":"","title":"Openhab"},{"location":"Containers/openHAB/#references","text":"Docker website openHAB has been added without Amazon Dashbutton binding. Port binding is 8080 for http and 8443 for https.","title":"References"},{"location":"Containers/x2go/","text":"x2go x2go is an \"alternative\" to using VNC for a remote connection. It uses X11 forwarding over ssh to provide a desktop environment Reason for using: I have a Pi 4 and I didn't buy a micro hdmi cable. You can use VNC however you are limited to a 800x600 window. Installation Install with sudo apt install x2goserver x2go cant connect to the native Raspbian Desktop so you will need to install another with sudo tasksel I chose Xfce because it is light weight. Install the x2go client from their website Now I have a full-screen client YouTube tutorial Laurence systems","title":"x2go"},{"location":"Containers/x2go/#x2go","text":"x2go is an \"alternative\" to using VNC for a remote connection. It uses X11 forwarding over ssh to provide a desktop environment Reason for using: I have a Pi 4 and I didn't buy a micro hdmi cable. You can use VNC however you are limited to a 800x600 window.","title":"x2go"},{"location":"Containers/x2go/#installation","text":"Install with sudo apt install x2goserver x2go cant connect to the native Raspbian Desktop so you will need to install another with sudo tasksel I chose Xfce because it is light weight. Install the x2go client from their website Now I have a full-screen client","title":"Installation"},{"location":"Containers/x2go/#youtube-tutorial","text":"Laurence systems","title":"YouTube tutorial"}]}